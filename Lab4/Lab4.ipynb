{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IagZMs0_qjdL"
   },
   "source": [
    "# **Lab 4 : Neural Network**\n",
    "\n",
    "In *lab 4*, you need to finish:\n",
    "\n",
    "1. Basic Part (65%):\n",
    "  Implement a deep neural network from scratch\n",
    "\n",
    "  > * Section 1: Neural network implementation\n",
    "    >> * Part 1: Linear layer\n",
    "    >> * Part 2: Activation function layer\n",
    "    >> * Part 3: Build model\n",
    "\n",
    "  > * Section 2: Loss function\n",
    "    >> * Part 1: Binary cross-entropy loss (BCE)\n",
    "    >> * Part 2: Categorical cross-entropy loss (CCE)\n",
    "    >> * Part 3: Mean square error (MSE)\n",
    "  > * Section 3: Training and prediction\n",
    "    >> * Part 1: Training function & batch function\n",
    "    >> * Part 2: Regression\n",
    "    >> * Part 3: Binary classification\n",
    "\n",
    "\n",
    "2. Advanced Part (30%): Multi class classification\n",
    "3. Report (5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGFR00CQvoaH"
   },
   "source": [
    "## **Important  notice**\n",
    "\n",
    "* Please **do not** change the code outside this code bracket in the basic part.\n",
    "  ```\n",
    "  ### START CODE HERE ###\n",
    "  ...\n",
    "  ### END CODE HERE ###\n",
    "  ```\n",
    "\n",
    "* Please **do not** import any other packages in both basic and advanced part\n",
    "\n",
    "* Please **do not** change the random seed **np.random.seed(1)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BgcgLVV79Bm"
   },
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fmTH9UkeqdYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO31dEFx-C1y"
   },
   "source": [
    "### Common Notation\n",
    "  * $C$: number of classes\n",
    "  * $n$: number of samples\n",
    "  * $f^{[l]}$: the dimension of outputs in layer $l$, but $f^{[0]}$ is the input dimension\n",
    "  * $Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}$\n",
    "      * $Z^{[l]}$: the output of layer $l$ in the shape $(n, f^{[l]})$\n",
    "      * $A^{[l]}$: the activation of $Z^{[l]}$ in the shape $(n, f^{[l]})$, but $A^{[0]}$ is input $X$\n",
    "      * $W^{[l]}$: the weight in layer $l$ in the shape $(f^{[l-1]}, f^{[l]})$\n",
    "      * $b^{[l]}$: the bias in layer $l$ in the shape $(1, f^{[l]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wE5z0w8FQLK"
   },
   "source": [
    "# **Basic Part (65%)**\n",
    "In the Basic Part, you will implement a neural network framework capable of handling both regression, binary classification and multi-class classification tasks.\n",
    "\n",
    "**Note:**\n",
    "After implementing each class/function, test it with the provided input variables to verify its correctness. Save the results in the **outputs** dictionary. (The code for testing and saving results is already provided.)\n",
    "## Section 1: Neural network implementation\n",
    "* Part 1: Linear layer\n",
    "> * Step 1: Linear Initialize parameters\n",
    "> * Step 2: Linear forward\n",
    "> * Step 3: Linear backward\n",
    "> * Step 4: Linear update parameters\n",
    "* Part 2: Activation function layer\n",
    "> * Step 1: Activation forward\n",
    "> * Step 2: Activation backward\n",
    "* Part 3: Build model\n",
    "> * Step 1: Model Initialize parameters\n",
    "> * Step 2: Model forward\n",
    "> * Step 3: Model backward\n",
    "> * Step 4: Model update parameters\n",
    "\n",
    "## Section 2: Loss function\n",
    "* Part 1: Binary cross-entropy loss (BCE)\n",
    "* Part 2: Categorical cross-entropy loss (CCE)\n",
    "* Part 3: Mean square error (MSE)\n",
    "\n",
    "## Section 3: Training and prediction\n",
    "* Part 1: Training function & batch function\n",
    "* Part 2: Regression\n",
    "* Part 3: Binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w35ZkTwMc00G"
   },
   "source": [
    "## **Section 1: Neural network implementation(30%)**\n",
    "To implement a neural network, you need to complete 3 classes: **Dense**, **Activation**, and **Model**.\n",
    "The process of training a deep neural network is composed of 3 steps: *forward propagation*, *backward propagation*, and *update*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_krGKUNg_Ix"
   },
   "source": [
    "## Part 1: Linear layer (10%)\n",
    "Dense layer (fully-connected layer) performs linear transformation:\n",
    "\n",
    "$Z = AW + b$, where W is weight matrix and b is bias vector.\n",
    "\n",
    "> ### Step 1: Initialize parameters (0%)\n",
    " * You don't need to write this part.\n",
    " * W is randomly initialized using uniform distribution within $[\\text\\{-limit\\}, \\text\\{limit\\}]$, where $\\text\\{limit\\} = \\sqrt{\\frac{6}{\\text\\{fanin\\} + \\text\\{fanout\\}}}$ (fanin: number of input features, fanout: number of output features)\n",
    " * b is initialized to 0\n",
    "\n",
    "> ### Step 2: Linear forward (4%)\n",
    "* Compute Z using matrix multiplication and addition\n",
    "\n",
    "> ### Step 3: Linear backward (4%)\n",
    "* Use backpropagation to compute gradients of loss function with respect to parameters\n",
    "* For layer l: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by activation)\n",
    "* Given $dZ^{[l]}$ (gradient of loss with respect to Z), we need to compute three gradients:\n",
    "  * $dW^{[l]}$: gradient of loss with respect to weights\n",
    "  * $db^{[l]}$: gradient of loss with respect to bias\n",
    "  * $dA^{[l-1]}$: gradient of loss with respect to previous layer output\n",
    "\n",
    "> Formulas:\n",
    "$$ dW^{[l]} = \\frac{1}{n} A^{[l-1] T} dZ^{[l]} $$\n",
    "$$ db^{[l]} = \\frac{1}{n} \\sum_{i = 1}^{n} dZ_i^{[l]} $$\n",
    "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$\n",
    "\n",
    "> ### Step 4: Linear update parameters (2%)\n",
    "* Update parameters using gradient descent:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x0KHo8w9yqbY"
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        self.n_x -- size of the input layer\n",
    "        self.n_y -- size of the output layer\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "                           W -- weight matrix of shape (n_x, n_y)\n",
    "                           b -- bias vector of shape (1, n_y)\n",
    "        \"\"\"\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "        Arguments:\n",
    "        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
    "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "        Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_forward\n",
    "        ### START CODE HERE ###\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = A @ W + b\n",
    "        self.cache = (A, W, b)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "        Arguments:\n",
    "        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
    "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "\n",
    "        \"\"\"\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # GRADED FUNCTION: linear_backward\n",
    "        ### START CODE HERE ###\n",
    "        self.dW = (A_prev.T @ dZ) / m\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dA_prev = dZ @ W.T\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "\n",
    "        Arguments:\n",
    "        learning rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "        self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbnVsi6VJMXD"
   },
   "source": [
    "### Test your **Dense class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7HNAWwmg8R7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]]\n",
      "b = [[0.]]\n",
      "Z = [[1.9]\n",
      " [2.2]\n",
      " [2.5]]\n",
      "dA_prev = [[3.5]\n",
      " [6. ]]\n",
      "dW = [[1.625 0.625]]\n",
      "db = [[2.   0.75]]\n",
      "W = [[0.5 2.5]]\n",
      "b = [[-1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "dense = Dense(3, 1)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "# Linear forward\n",
    "A, W, b = np.array([[0., 1., 2.], [0.5, 1.5, 2.5], [1., 2., 3.]]), np.array([[0.1], [0.2], [0.3]]), np.array([[1.1]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "print(\"Z = \" + str(Z))\n",
    "\n",
    "A, W, b = np.array([[-0.80,-0.45,-1.11],[-1.65,-2.36,1.14],[-1.02,0.64,-0.86]]), np.array([[0.3], [0.3], [0.1]]), np.array([[-6.2]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "outputs[\"dense_forward\"] = (Z, dense.cache)\n",
    "\n",
    "# Linear backward\n",
    "dZ, linear_cache = np.array([[1.5, 0.5], [2.5, 1.]]), (np.array([[0.5], [1]]), np.array([[2., 1.0]]), np.array([[0.5, 1.]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "print (\"dA_prev = \" + str(dA_prev))\n",
    "print (\"dW = \" + str(dense.dW))\n",
    "print (\"db = \" + str(dense.db))\n",
    "\n",
    "dZ, linear_cache = np.array([[0.52,0.34],[0.76,0.89]]), (np.array([[0.42], [0.68]]), np.array([[0.35, 0.89]]), np.array([[0.12, 0.76]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "outputs[\"dense_backward\"] = (dA_prev, dense.dW, dense.db)\n",
    "\n",
    "# Linear update parameters\n",
    "np.random.seed(1)\n",
    "dense = Dense(1, 2)\n",
    "dense.parameters = {\"W\": np.array([[1.0, 2.0]]), \"b\": np.array([[0.5, 0.5]])}\n",
    "dense.dW = np.array([[0.5, -0.5]])\n",
    "dense.db = np.array([[1.5, -1.5]])\n",
    "dense.update(1.0)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "dense = Dense(3, 4)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(1,4)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(1,4)}\n",
    "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "dense.dW = grads[\"dW1\"]\n",
    "dense.db = grads[\"db1\"]\n",
    "dense.update(0.1)\n",
    "outputs[\"dense_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPtH0j3BFN7"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[-0.20325375]  [0.53968259 [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Z: </td>\n",
    "    <td>[[1.9] [2.2] [2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev: </td>\n",
    "    <td>[[3.5] [6.0]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW: </td>\n",
    "    <td>[[1.625 0.625]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db: </td>\n",
    "    <td>[[2.0 0.75]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[0.5 2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[-1.  2.]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r5m2W3aXh_A"
   },
   "source": [
    "## Part 2: Activation function layer (10%)\n",
    "\n",
    "Implement forward and backward propagation for activation function layers, including Sigmoid, Softmax, and ReLU.\n",
    "\n",
    "> ### Step 1: Forward Propagation (5%)\n",
    " Implement the following activation functions:\n",
    ">> #### a) Sigmoid\n",
    "- Use the numerically stable version to prevent exponential overflow:\n",
    "  $$\\sigma(Z) = \\begin{cases}\n",
    "    \\frac{1}{1+e^{-Z}},& \\text{if } Z \\geq 0\\\\\n",
    "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "\n",
    ">> #### b) ReLU\n",
    "- Simple implementation:\n",
    "  $$RELU(Z) = \\max(Z, 0)$$\n",
    "\n",
    ">> #### c) Softmax\n",
    "- Implement using the numerically stable version:\n",
    "  $$\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{C} e^{Z_j-b}}$$\n",
    "  where $b = \\max_{j=1}^{C} Z_j$\n",
    "\n",
    ">> #### d) Linear\n",
    "- You don't need to implement this part\n",
    "\n",
    "> ### Requirements\n",
    "- Each function should return:\n",
    "  1. Activation value \"a\"\n",
    "  2. Cache containing \"z\" for backward propagation\n",
    "\n",
    "> ### Step 2: Backward Propagation (5%)\n",
    "Implement backward functions for:\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- Softmax\n",
    "- linear\n",
    "\n",
    "> ### General Form\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "where $g(.)$ is the activation function\n",
    "\n",
    "> ### Specific Implementations\n",
    "\n",
    ">> #### a) Sigmoid Backward\n",
    "$$\\sigma'(Z^{[l]}) = \\sigma(Z^{[l]}) (1 - \\sigma(Z^{[l]}))$$\n",
    "Use numerically stable sigmoid\n",
    "\n",
    ">> #### b) ReLU Backward\n",
    "$$g'(Z^{[l]}) = \\begin{cases}\n",
    "    1,& \\text{if } Z^{[l]} > 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    ">> #### c) Softmax Backward\n",
    "For the special case of Softmax combined with Categorical Cross-Entropy loss:\n",
    "$$dZ^{[l]} = s - y$$\n",
    "where $s$ is softmax output, $y$ is true label (one-hot vector)\n",
    "\n",
    "Note: This is a simplified form specific to Softmax + CCE loss combination.\n",
    "\n",
    ">> #### d) linear Backward\n",
    "You don't need to implement this part\n",
    "\n",
    "> ### Note\n",
    "For softmax, use the normalized exponential function to prevent overflow, but use the simplified gradient equation for backwards propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nnuv8MmebMgg"
   },
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implements the sigmoid activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of sigmoid(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_forward\n",
    "            ### START CODE HERE ###\n",
    "            def sigmoid(z):\n",
    "                res = np.zeros(z.shape)\n",
    "                pos_mask = z >= 0\n",
    "                res[pos_mask] = 1 / (1 + np.exp(-z[pos_mask]))\n",
    "                \n",
    "                neg_mask = ~pos_mask\n",
    "                exp_term = np.exp(z[neg_mask])\n",
    "                res[neg_mask] = exp_term / (1 + exp_term)\n",
    "                return res\n",
    "            A = sigmoid(Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the RELU function in numpy\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "            Returns:\n",
    "            A -- output of relu(z), same shape as Z\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_forward\n",
    "            ### START CODE HERE ###\n",
    "            def relu(z):\n",
    "                return np.maximum(z, 0)\n",
    "            A = relu(Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implements the softmax activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- np.array with shape (n, C)\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of softmax(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_forward\n",
    "            ### START CODE HERE ###\n",
    "            def softmax(z):\n",
    "                shifted_z = z - np.max(z, axis=1, keepdims=True)\n",
    "                exp_scores = np.exp(shifted_z)\n",
    "                return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            A = softmax(Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Linear activation (returns Z directly).\n",
    "            \"\"\"\n",
    "            self.cache = Z.copy()\n",
    "            return Z\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single SIGMOID unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_backward\n",
    "            ### START CODE HERE ###\n",
    "            def sigmoid(z):\n",
    "                res = np.zeros(z.shape)\n",
    "                pos_mask = z >= 0\n",
    "                res[pos_mask] = 1 / (1 + np.exp(-z[pos_mask]))\n",
    "                \n",
    "                neg_mask = ~pos_mask\n",
    "                exp_term = np.exp(z[neg_mask])\n",
    "                res[neg_mask] = exp_term / (1 + exp_term)\n",
    "                return res\n",
    "            def sigmoid_derivative(z):\n",
    "                s = sigmoid(z)\n",
    "                res = s * (1 - s)\n",
    "                return res\n",
    "            Z = self.cache\n",
    "            dZ = dA * sigmoid_derivative(Z)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single RELU unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_backward\n",
    "            ### START CODE HERE ###\n",
    "            def relu_derivative(z):\n",
    "                res = np.zeros(z.shape)\n",
    "                mask = z > 0\n",
    "                res[mask] = 1\n",
    "                res[~mask] = 0\n",
    "                return res\n",
    "            Z = self.cache\n",
    "            dZ = dA * relu_derivative(Z) + 0\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
    "            Arguments:\n",
    "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the cost with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_backward\n",
    "            ### START CODE HERE ###\n",
    "            def softmax(z):\n",
    "                shifted_z = z - np.max(z, axis=1, keepdims=True)\n",
    "                exp_scores = np.exp(shifted_z)\n",
    "                return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            Z = self.cache\n",
    "            s = softmax(Z)\n",
    "            dZ = s - Y\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Backward propagation for linear activation.\n",
    "            \"\"\"\n",
    "            return dA\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYVMMS2ecCx"
   },
   "source": [
    "### Test your **Activation class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gBuRAoeUC5jV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: A = [[0.00669285]\n",
      " [0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.99330715]]\n",
      "ReLU: A = [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "Softmax: A = \n",
      "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
      "Linear: A = \n",
      "[[ 1  2  3  4]\n",
      " [ 0  1  0  0]\n",
      " [-2 -1  0  1]]\n",
      "Sigmoid: dZ = [[-0.5       ]\n",
      " [-0.26935835]\n",
      " [-0.11969269]\n",
      " [-0.5       ]\n",
      " [-0.73139639]]\n",
      "ReLU: dZ = [[ 0.    1.7 ]\n",
      " [ 0.    0.  ]\n",
      " [-1.14  3.72]]\n",
      "Softmax: dZ = [[-0.96488097  0.70538451  0.25949646]\n",
      " [ 0.09003057 -0.75527153  0.66524096]\n",
      " [ 0.01766842  0.01766842 -0.03533684]]\n",
      "Linear: dZ = \n",
      "[[ 1.2 -0.5  0.8 -0.3]\n",
      " [ 0.4  0.6 -0.9  0.2]\n",
      " [-0.1  0.5 -0.7  0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Activation forward\n",
    "Z = np.array([[-5], [-1], [0], [1], [5]])\n",
    "\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "A = sigmoid.forward(Z)\n",
    "print(\"Sigmoid: A = \" + str(A))\n",
    "A = sigmoid.forward(np.array([[0.23], [-0.67], [0.45], [0.89], [-0.10]]))\n",
    "outputs[\"sigmoid\"] = (A, sigmoid.cache)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "A = relu.forward(Z)\n",
    "print(\"ReLU: A = \" + str(A))\n",
    "A = relu.forward(np.array([[-0.34], [-0.76], [0.21], [-0.98], [0.54]]))\n",
    "outputs[\"relu\"] = (A, relu.cache)\n",
    "\n",
    "Z = np.array([[1, 2, 3, 4],[0, 1, 0, 0],[-2, -1, 0, 1]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "A = softmax.forward(Z)\n",
    "print(\"Softmax: A = \\n\" + str(A))\n",
    "A = softmax.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"softmax\"] = (A, softmax.cache)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "A = linear.forward(Z)\n",
    "print(\"Linear: A = \\n\" + str(A))\n",
    "A = linear.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"linear\"] = (A, Z)  # For linear activation, cache is just Z\n",
    "\n",
    "# Activation backward\n",
    "dA, cache = np.array([[-2], [-1.37], [-1.14], [-2], [-3.72]]), np.array([[0], [1], [2], [0], [1]])\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "sigmoid.cache = cache\n",
    "dZ = sigmoid.backward(dA=dA)\n",
    "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[9.73], [-7.56], [8.34], [-4.12], [6.89]]), np.array([[-5.45], [3.68], [-2.32], [4.51], [-9.27]])\n",
    "sigmoid.cache = cache\n",
    "outputs[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "dA, cache = np.array([[-2., 1.7 ], [-1.37, 2.], [-1.14, 3.72]]), np.array([[-2, 1], [-1, 0], [2, 1]])\n",
    "relu.cache = cache\n",
    "dZ = relu.backward(dA=dA)\n",
    "print(\"ReLU: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[7.24, -3.58], [8.93, 6.45], [-2.11, 9.87]]), np.array([[-4.76, 5.34], [1.98, -7.22], [3.67, -8.56]])\n",
    "relu.cache = cache\n",
    "outputs[\"relu_backward\"] = relu.backward(dA=dA)\n",
    "\n",
    "Y, cache = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "softmax.cache = cache\n",
    "dZ = softmax.backward(Y=Y)\n",
    "print(\"Softmax: dZ = \" + str(dZ))\n",
    "Y, cache = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]]), np.array([[-9.45, 7.32, 3.58], [5.61, -8.27, 6.49], [1.23, -4.56, 7.84]])\n",
    "softmax.cache = cache\n",
    "outputs[\"softmax_backward\"] = softmax.backward(Y=Y)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "dA = np.array([[1.2, -0.5, 0.8, -0.3], [0.4, 0.6, -0.9, 0.2], [-0.1, 0.5, -0.7, 0.9]])\n",
    "dZ = linear.backward(dA=dA)\n",
    "print(\"Linear: dZ = \\n\" + str(dZ))\n",
    "outputs[\"linear_backward\"] = dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyyX_xxdEmNp"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Sigmoid: A</td>\n",
    "    <td>[[0.00669285] [0.26894142] [0.5] [0.73105858] [0.99330715]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReLU: A</td>\n",
    "    <td>[[0] [0] [0] [1] [5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Softmax: A</td>\n",
    "    <td>\n",
    "      [[0.0320586 0.08714432 0.23688282 0.64391426]\n",
    "       [0.1748777 0.47536689 0.1748777 0.1748777]\n",
    "       [0.0320586 0.08714432 0.23688282 0.64391426]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Linear: A</td>\n",
    "    <td>\n",
    "      [[1 2 3 4]\n",
    "       [0 1 0 0]\n",
    "       [-2 -1 0 1]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Sigmoid) dZ</td>\n",
    "    <td>[[-0.5] [-0.26935835] [-0.11969269] [-0.5] [-0.73139639]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with ReLU) dZ</td>\n",
    "    <td>[[0 1.7] [0 0] [-1.14 3.72]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Softmax) dZ</td>\n",
    "    <td>\n",
    "      [[-0.96488097 0.70538451 0.25949646]\n",
    "       [0.09003057 -0.75527153 0.66524096]\n",
    "       [0.01766842 0.01766842 -0.03533684]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Linear) dZ</td>\n",
    "    <td>\n",
    "      [[1.2 -0.5 0.8 -0.3]\n",
    "       [0.4 0.6 -0.9 0.2]\n",
    "       [-0.1 0.5 -0.7 0.9]]\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vcTYp_yoPu"
   },
   "source": [
    "## Part 3: Model (10%)\n",
    "\n",
    "Use the functions that you had previously written to implement the complete neural network model, including initialization, forward propagation, backward propagation, and parameter updates.\n",
    "\n",
    "> ### Step 1: Model Initialization (0%)\n",
    "Initialize the model by creating linear and activation function layers.\n",
    "\n",
    ">> #### Requirements:\n",
    "- Store linear layers in a list called `linear`\n",
    "- Store activation function layers in a list called `activation`\n",
    "- Use iteration number as seed for each Dense layer initialization\n",
    "\n",
    ">> #### Note:\n",
    "A linear-activation pair counts as a single layer in the neural network.\n",
    "\n",
    "> ### Step 2: Forward Propagation (4%)\n",
    "Implement the model's forward pass by calling each layer's forward function sequentially.\n",
    "\n",
    ">> #### Process:\n",
    "1. For layers 1 to N-1: [LINEAR -> ACTIVATION]\n",
    "2. Final layer: LINEAR -> SIGMOID (binary) or SOFTMAX (multi-class)\n",
    "\n",
    ">> #### Note:\n",
    "For binary classification, use one output node with sigmoid activation. For K-class classification, use K output nodes with softmax activation.\n",
    "\n",
    "> ### Step 3: Backward Propagation (4%)\n",
    "Implement the model's backward pass by calling each layer's backward function in reverse order.\n",
    "\n",
    ">> #### Process:\n",
    "1. Initialize backpropagation:\n",
    "   - Regression:\n",
    "     $$dAL = AL - Y$$\n",
    "   - Binary classification:\n",
    "     $$dAL = - (\\frac{Y}{AL + \\epsilon} - \\frac{1 - Y}{1 - AL + \\epsilon})$$\n",
    "     where $\\epsilon = 10^{-5}$ to prevent division by zero\n",
    "   - Multi-class classification:\n",
    "     Use `softmax_backward` function\n",
    "2. Backpropagate through layers L to 1\n",
    "\n",
    ">> #### Note:\n",
    "Use cached values from the forward pass in each layer's backward function.\n",
    "\n",
    "> ### Step 4: Parameter Update (2%)\n",
    "Update model parameters using gradient descent.\n",
    "\n",
    ">> #### Update Rule:\n",
    "For each layer $l = 1, 2, ..., L$:\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
    "where $\\alpha$ is the learning rate\n",
    "\n",
    "This revised structure provides a clear, step-by-step breakdown of the model implementation process, mirroring the format used in Part 2. It covers all the essential components while maintaining a concise and logical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0JGMzfIDCSVz"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize layers of the neural network\n",
    "\n",
    "        Arguments:\n",
    "            self.units -- array defining network structure (e.g., [4,4,1]):\n",
    "                - Input layer: 4 nodes\n",
    "                - Hidden layer: 4 nodes\n",
    "                - Output layer: 1 node\n",
    "            self.activation_functions -- activation function for each layer (e.g., [\"relu\",\"sigmoid\"]):\n",
    "                - First layer uses ReLU\n",
    "                - Second layer uses Sigmoid\n",
    "            self.loss_function -- loss function type: \"cross_entropy\" or \"mse\"\n",
    "        \"\"\"\n",
    "        self.linear = []        # Store all Dense layers (weights & biases)\n",
    "        self.activation = []    # Store all activation function layers\n",
    "\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "\n",
    "        Arguments:\n",
    "        X -- input data: shape (n, f)\n",
    "        Returns:\n",
    "        A -- model output:\n",
    "            - For binary classification: probability (0-1)\n",
    "            - For multi-class: probability distribution across classes\n",
    "            - For regression: predicted values\n",
    "        \"\"\"\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ###\n",
    "        n_layer = len(self.linear)\n",
    "        for i in range(n_layer):\n",
    "            A = self.linear[i].forward(A)\n",
    "            A = self.activation[i].forward(A)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients\n",
    "\n",
    "        Arguments:\n",
    "            AL -- model output from forward propagation:\n",
    "                - For binary: probability (n,1)\n",
    "                - For multi-class: probabilities (n,C)\n",
    "            Y -- true labels:\n",
    "                - For binary: 0/1 labels (n,1)\n",
    "                - For multi-class: one-hot vectors (n,C)\n",
    "                - For regression: true values (n,1)\n",
    "\n",
    "        Returns:\n",
    "            dA_prev -- gradients for previous layer's activation\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            if Y.shape[1] == 1:  # binary classification\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "            else:  # multi-class classification\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "        elif self.loss_function == \"mse\":\n",
    "            assert self.activation_functions[-1] == 'linear', warning\n",
    "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "        # GRADED FUNCTION: model_backward\n",
    "        ### START CODE HERE ###\n",
    "        if self.activation_functions[-1] == \"linear\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = AL - Y\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dAL, Y)\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"sigmoid\":\n",
    "            # Initializing the backpropagation\n",
    "            eps = 1e-5\n",
    "            dAL = - (Y/(AL+eps) - (1-Y)/(1-AL+eps))\n",
    "\n",
    "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dAL, Y)\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "            # Initializing the backpropagation\n",
    "            dZ = self.activation[-1].backward(Y=Y)\n",
    "\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        i = len(self.linear) - 2\n",
    "        while i >= 0:\n",
    "            dZ = self.activation[i].backward(dA_prev, Y)\n",
    "            dA_prev = self.linear[i].backward(dZ)\n",
    "            i -= 1\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        learning_rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # GRADED FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(L):\n",
    "            self.linear[i].update(learning_rate)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxQtZMmA1SNc"
   },
   "source": [
    "### Test your **Model class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EGY7_1bjcm-c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:  [[ 0.09762701  0.08976637 -0.12482558]\n",
      " [ 0.43037873 -0.1526904   0.783546  ]\n",
      " [ 0.20552675  0.29178823  0.92732552]] \n",
      "b1:  [[0. 0. 0.]]\n",
      "W2:  [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]] \n",
      "b2:  [[0.]]\n",
      "With sigmoid: A = [[0.64565631]\n",
      " [0.20915937]\n",
      " [0.77902611]]\n",
      "With ReLU: A = [[0.6 ]\n",
      " [0.  ]\n",
      " [1.26]]\n",
      "With softmax: A = \n",
      "[[0.47535001 0.14317267 0.38147732]\n",
      " [0.05272708 0.75380161 0.19347131]\n",
      " [0.68692136 0.05526942 0.25780921]]\n",
      "AL = [[0.56058713]\n",
      " [0.55220559]\n",
      " [0.46331713]]\n",
      "Length of layers list = 2\n",
      "AL = [[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001\n",
      "  0.08448599 0.09067641 0.1294968  0.08303407]\n",
      " [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785\n",
      "  0.08664355 0.09207969 0.12512634 0.08536063]\n",
      " [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299\n",
      "  0.08734059 0.12452515 0.13002144 0.08467232]]\n",
      "Length of layers list = 2\n"
     ]
    }
   ],
   "source": [
    "# Model initialize parameters\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
    "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])\n",
    "\n",
    "# Model forward\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[4.35, -5.67], [-7.89, 8.12]]), np.array([[-3.54], [-2.34]]), np.array([[0.8]])\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[7.23, -4.56], [5.67, -8.90]]), np.array([[-9.12], [3.45]]), np.array([[0.25]])\n",
    "model = Model([2, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1, -0.1, -0.1],[0.2, -0.2, 0.],[0.3, -0.3, 0.1]]), np.array([[-0.5, 0.5, 0.1]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With softmax: A = \\n\" + str(A))\n",
    "A_prev, W, b = np.array([[-5.12, 4.56, 7.89], [8.34, -6.78, 2.45], [3.21, -4.67, 5.98]]), np.array([[6.23, -7.85, 4.56], [-3.21, 9.87, -2.34], [1.23, -5.67, 8.90]]), np.array([[4.12, -6.54, 7.89]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "# binary classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))\n",
    "\n",
    "# multi-class classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 10], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEmggOxtdMnl"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.09762701 0.08976637 -0.12482558] [ 0.43037873 -0.1526904 0.783546 ] [ 0.20552675 0.29178823 0.92732552]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[0. 0. 0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[-0.20325375] [ 0.53968259] [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Sigmoid:</td>\n",
    "    <td>A = [[0.64565631] [0.20915937] [0.77902611]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With ReLU:</td>\n",
    "    <td>A = [[0.6 ] [0. ] [1.26]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Softmax:</td>\n",
    "    <td>A = [[0.47535001 0.14317267 0.38147732] [0.05272708 0.75380161 0.19347131] [0.68692136 0.05526942 0.25780921]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.56058713] [0.55220559] [0.46331713]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001 0.08448599 0.09067641 0.1294968  0.08303407]\n",
    "         [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785 0.08664355 0.09207969 0.12512634 0.08536063]\n",
    "         [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299 0.08734059 0.12452515 0.13002144 0.08467232]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HOGsyLXPNGh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.55554938  0.27777469]\n",
      " [ 0.49152369  0.24576184]\n",
      " [-0.41996594 -0.20998297]\n",
      " [-0.55554938 -0.27777469]\n",
      " [-0.39321993 -0.19660997]]\n",
      "dW = [[-0.29446117]\n",
      " [ 0.29446117]]\n",
      "db = [[-0.03216622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[-0.01269296 -0.05595562]\n",
      " [ 0.01470136  0.06480946]\n",
      " [ 0.          0.        ]\n",
      " [-0.07496777 -0.0327431 ]\n",
      " [-0.07151883 -0.03123674]]\n",
      "dW = [[ 0.0178719  -0.17321413]\n",
      " [-0.0178719   0.17321413]]\n",
      "db = [[ 0.00335943 -0.11638953]]\n",
      "\n",
      "Binary classification\n",
      "dW1 = [[-0.06277946  0.26602938 -0.37820327]\n",
      " [ 0.          0.05875647  0.        ]\n",
      " [-0.01569486  0.05181823 -0.09455082]]\n",
      "db1 = [[-0.03138973  0.10363646 -0.18910163]]\n",
      "dA_prev = [[-0.02128713  0.03620889 -0.06919444]\n",
      " [ 0.02675119 -0.04550313  0.08695554]\n",
      " [ 0.08406585 -0.52321654 -0.47247201]]\n",
      "\n",
      "Multi-class classification\n",
      "dW1 = [[ 0.16593371  0.33171007 -0.32297709]\n",
      " [ 0.          0.15006987  0.        ]\n",
      " [ 0.04148343  0.04541005 -0.08074427]]\n",
      "db1 = [[ 0.08296685  0.0908201  -0.16148854]]\n",
      "dA_prev = [[-0.04735391  0.08054785 -0.15392528]\n",
      " [ 0.05429414 -0.09235301  0.1764847 ]\n",
      " [ 0.10229066 -0.30227651 -0.34116033]]\n",
      "\n",
      "Regression\n",
      "dW1 = [[ 0.45352627 -1.49031638  2.73218534]\n",
      " [ 0.          1.09795245  0.        ]\n",
      " [ 0.11338157 -0.64706721  0.68304634]]\n",
      "db1 = [[ 0.22676313 -1.29413441  1.36609267]]\n",
      "dA_prev = [[-0.10931473  0.18594169 -0.35533076]\n",
      " [-0.07704814  0.13105702 -0.25044727]\n",
      " [-0.60730166  3.77977844  3.41319394]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model backward\n",
    "AL, Y, linear_activation_cache = np.array([[0.1], [0.2], [0.5], [0.9], [1.0]]), np.array([[0], [0], [1], [1], [1]]), (((np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[2.0], [1.0]]), np.array([[0.5]])), np.array([[0], [1], [2], [0], [1]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "AL, Y, linear_activation_cache = np.array([[0.35], [0.93], [0.23], [0.72], [0.90]]), np.array([[1], [0], [1], [0], [1]]), (((np.array([[-1, 2], [1, 3], [2, 0], [1, -4], [3, -2]]), np.array([[1.7], [3.2]]), np.array([[0.25]])), np.array([[2], [1], [2], [0], [0]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "X, Y = np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[0], [1], [1], [1], [1]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "X, Y = np.array([[4.56, -3.21], [-7.85, 6.34], [2.45, -8.90], [5.67, 3.12], [-4.78, 7.89]]), np.array([[1], [1], [0], [1], [0]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "# binary classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1], [0], [0]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Binary classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# multi-class classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Multi-class classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# regression - mse\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[2.5], [1.8], [3.2]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"linear\"], \"mse\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Regression\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6xzEk3-NGh6"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Sigmoid</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[ 0.55554938  0.27777469] [ 0.49152369  0.24576184] [-0.41996594 -0.20998297] [-0.55554938 -0.27777469] [-0.39321993 -0.19660997]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[-0.29446117] [ 0.29446117]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[-0.03216622]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">ReLU</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.01269296 -0.05595562] [ 0.01470136  0.06480946] [ 0.  0. ] [-0.07496777 -0.0327431 ] [-0.07151883 -0.03123674]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[ 0.0178719  -0.17321413] [-0.0178719   0.17321413]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[ 0.00335943 -0.11638953]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Binary Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[-0.06277946  0.26602938 -0.37820327] [ 0.  0.05875647  0. ] [-0.01569486  0.05181823 -0.09455082]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[-0.03138973  0.10363646 -0.18910163]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.02128713  0.03620889 -0.06919444] [ 0.02675119 -0.04550313  0.08695554] [ 0.08406585 -0.52321654 -0.47247201]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Multi-class Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.16593371  0.33171007 -0.32297709] [ 0.  0.15006987  0. ] [ 0.04148343  0.04541005 -0.08074427]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.08296685  0.0908201  -0.16148854]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.04735391  0.08054785 -0.15392528] [ 0.05429414 -0.09235301  0.1764847 ] [ 0.10229066 -0.30227651 -0.34116033]]</td>\n",
    "  </tr>\n",
    "  <th colspan=\"2\">Regression</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.45352627 -1.49031638  2.73218534] [ 0.          1.09795245  0.        ] [ 0.11338157 -0.64706721  0.68304634]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.22676313 -1.29413441  1.36609267]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.10931473  0.18594169 -0.35533076] [-0.07704814  0.13105702 -0.25044727] [-0.60730166  3.77977844  3.41319394]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qoGA4O8BUCvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.39721186  0.07752363  0.392862  ]\n",
      " [ 0.64025004  0.00469968  0.52183369]\n",
      " [-0.09671178  0.09679955  0.33138026]\n",
      " [ 0.27099015  0.33705631  0.67538482]]\n",
      "b1 = [[ 0.16234149  0.78232848 -0.02592894]]\n",
      "W2 = [[0.6012798 ]\n",
      " [0.38575324]\n",
      " [0.49003974]]\n",
      "b2 = [[0.05692437]]\n"
     ]
    }
   ],
   "source": [
    "# Model update\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.1)\n",
    "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
    "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
    "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
    "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.075)\n",
    "outputs[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t-HfnHZWYIa"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Data Representation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.39721186  0.07752363  0.392862 ] [ 0.64025004  0.00469968  0.52183369] [-0.09671178  0.09679955  0.33138026] [ 0.27099015  0.33705631  0.67538482]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[ 0.16234149  0.78232848 -0.02592894]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[0.6012798 ] [0.38575324] [0.49003974]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.05692437]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSBVaQOSRrk"
   },
   "source": [
    "## **Section 2: Loss function(10%)**\n",
    "In this section, you need to implement the loss function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScdQdj85uC0P"
   },
   "source": [
    "## Part 1: Binary cross-entropy loss (BCE) (5%)\n",
    "Compute the binary cross-entropy loss $L$, using the following formula:  $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+ϵ\\right)), where\\ ϵ=1e-5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MjBT0eYQaY81"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_BCE_loss\n",
    "\n",
    "def compute_BCE_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the binary cross-entropy loss function using the above formula.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (n, 1)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (n, 1)\n",
    "\n",
    "    Returns:\n",
    "    loss -- binary cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = -(1/n) * np.sum(Y * np.log(AL + 1e-5) + (1 - Y) * np.log(1 - AL + 1e-5))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoV03IzimBEN"
   },
   "source": [
    "### Test your **compute_BCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r07sqnIXaaMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.5783820772863568\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.9], [0.6], [0.4], [0.1], [0.2], [0.8]]), np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "print(\"loss = \" + str(compute_BCE_loss(AL, Y)))\n",
    "outputs[\"compute_BCE_loss\"] = compute_BCE_loss(np.array([[0.12], [0.85], [0.47], [0.33], [0.76], [0.58], [0.09], [0.62]]), np.array([[1], [1], [0], [1], [0], [1], [1], [0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iRtgOx_IGPo"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.5783820772863568</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aealRyKbcQzG"
   },
   "source": [
    "## Part 2: Categorical cross-entropy loss (CCE) (5%)\n",
    "Compute the categorical cross-entropy loss $L$, using the following formula: $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right)),\\ ϵ = 1e-5$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Owx-kTdcfxV5"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_CCE_loss\n",
    "\n",
    "def compute_CCE_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the categorical cross-entropy loss function using the above formula.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (n, C)\n",
    "    Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "\n",
    "    Returns:\n",
    "    loss -- categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = -(1/n) * np.sum(Y * np.log(AL + 1e-5))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSOsacYQmNAb"
   },
   "source": [
    "### Test your **compute_CCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0YbHVAc7hSh3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4722526144672341\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.8, 0.1, 0.1],[0.6, 0.3, 0.1],[0.4, 0.5, 0.1],[0.1, 0.7, 0.2],[0.2, 0.1, 0.7],[0.4, 0.1, 0.5]]), np.array([[1, 0, 0],[1, 0, 0],[0, 1, 0],[0, 1, 0],[0, 0, 1],[0, 0, 1]])\n",
    "print(\"loss = \" + str(compute_CCE_loss(AL, Y)))\n",
    "outputs[\"compute_CCE_loss\"] = compute_CCE_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VVIBB5Ic-D"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.4722526144672341</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_XIpJtBpiAX"
   },
   "source": [
    "## Part 3: Mean square error (MSE) (0%)\n",
    "You don't need to write this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6RHLNGsepygt"
   },
   "outputs": [],
   "source": [
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8_AYhzfqlbg"
   },
   "source": [
    "## **Section 3: Training and prediction(35%)**\n",
    "In this section, you will apply your implemented neural network to regression and binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpFQpiK5eF64"
   },
   "source": [
    "## Helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "woCqucFUYXe6"
   },
   "outputs": [],
   "source": [
    "def predict(x, y_true, model):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    x -- data set of examples you would like to label\n",
    "    model -- trained model\n",
    "\n",
    "    Returns:\n",
    "    y_pred -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Forward propagation\n",
    "    y_pred = model.forward(x)\n",
    "\n",
    "    # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
    "    # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
    "    # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
    "    if y_pred.shape[-1] == 1:\n",
    "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "        if y_true is not None:\n",
    "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "    # make y_pred/y_true become one-hot prediction result\n",
    "    # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
    "    # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
    "    if y_true is not None:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    if y_true is not None:\n",
    "        # compute accuracy\n",
    "        correct = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == yp:\n",
    "                correct += 1\n",
    "        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "        print(f'f1 score for each class: {f1_scores}')\n",
    "        print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "    # Create DataFrame with ID, x, and y columns\n",
    "    df = pd.DataFrame({\n",
    "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "        'y': predicted_y\n",
    "    })\n",
    "\n",
    "    # Ensure ID is the first column\n",
    "    df = df[['ID', 'y']]\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "    # show current epoch on the animation / 100 epoch\n",
    "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        epoch_text.set_text('')\n",
    "        return line, epoch_text\n",
    "\n",
    "    def update(frame):\n",
    "        epoch = (frame + 1) * 100\n",
    "        _, predicted_y = history[frame]\n",
    "        predicted_x = X_train.flatten()\n",
    "        line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "        epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "        return line, epoch_text\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "    # save as gif\n",
    "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "    AL = model.forward(X_train)\n",
    "\n",
    "    predicted_x = X_train.flatten()\n",
    "    predicted_y = AL.flatten()\n",
    "\n",
    "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "\n",
    "    save_prediction_data(predicted_y)\n",
    "\n",
    "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(0, 11)\n",
    "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "    plt.show()\n",
    "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgVRVmOYG9FK"
   },
   "source": [
    "## Part1: Training function & batch function (5%)\n",
    "The functions defined in this part will be utilized in the subsequent training parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjOBHI0bGVE7"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n, f^{0})\n",
    "    Y -- true \"label\" vector, of shape (n, C)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "    \"\"\"\n",
    "    Trains the model using mini-batch gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    model -- the model to be trained\n",
    "    X_train -- training set, of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels, of shape (1, m_train)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    batch_size -- size of a mini batch\n",
    "    print_loss -- if True, print the loss every print_freq iterations\n",
    "    print_freq -- print frequency\n",
    "    decrease_freq -- learning rate decrease frequency\n",
    "    decrease_proportion -- learning rate decrease proportion\n",
    "\n",
    "    Returns:\n",
    "    model -- the trained model\n",
    "    losses -- list of losses computed during the optimization\n",
    "    history -- list of (X_train, Y_pred) tuples for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    history = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ### START CODE HERE ###\n",
    "        # Define mini batches\n",
    "        if batch_size:\n",
    "            mini_batches = random_mini_batches(X=X_train, Y=Y_train, mini_batch_size=batch_size)\n",
    "        else:\n",
    "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "            mini_batches = np.array(((X_train, Y_train),))\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in mini_batches:\n",
    "            X_batch, Y_batch = batch\n",
    "            # Forward pass\n",
    "            AL = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            if model.loss_function == 'cross_entropy':\n",
    "                if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "                    loss = compute_BCE_loss(AL, Y_batch)\n",
    "                elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "                    loss = compute_CCE_loss(AL, Y_batch)\n",
    "            elif model.loss_function == 'mse': # Regression\n",
    "                loss = compute_MSE_loss(AL, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            dA_prev = model.backward(AL=AL, Y=Y_batch)\n",
    "\n",
    "            # Update parameters\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        epoch_loss /= len(mini_batches)\n",
    "        losses.append(epoch_loss)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print loss\n",
    "        if print_loss and i % print_freq == 0:\n",
    "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "        # Store history\n",
    "        if i % 100 == 0:\n",
    "            history.append((X_train, model.forward(X_train)))\n",
    "\n",
    "        # Decrease learning rate\n",
    "        if i % decrease_freq == 0 and i > 0:\n",
    "            learning_rate *= decrease_proportion\n",
    "\n",
    "    return model, losses, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2D8lubAw8pX"
   },
   "source": [
    "## Part 2: Regression (10%)\n",
    "In this part, Your task is to train a neural network model to approximate the following mathematical function:\n",
    "\n",
    "$$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-ksy7v-Hrrt"
   },
   "source": [
    "> ### Step 1: Data generation\n",
    "Generate the mathematical function :  $$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0yRO6y_FyLPM"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_points=1000):\n",
    "\n",
    "    x = np.linspace(0.01, 11, num_points)\n",
    "    y = np.sin(2 * np.sin(2 * np.sin(2 * np.sin(x))))\n",
    "\n",
    "    return x.reshape(-1, 1), y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC_BxLJJHxHD"
   },
   "source": [
    "> ### Step 2: Train model\n",
    "Implement and train your model using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "alsJ4F6eHZ2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "Loss after iteration 0: 0.5841173152912486\n",
      "Loss after iteration 1000: 0.4822729708702959\n",
      "Loss after iteration 2000: 0.4115031087424418\n",
      "Loss after iteration 3000: 0.3599012124257031\n",
      "Loss after iteration 4000: 0.32082684996437427\n",
      "Loss after iteration 5000: 0.29010645620644393\n",
      "Loss after iteration 6000: 0.27284965628823\n",
      "Loss after iteration 7000: 0.2577437104099844\n",
      "Loss after iteration 8000: 0.24436284434601738\n",
      "Loss after iteration 9000: 0.2324284498310433\n",
      "Loss after iteration 10000: 0.22169031266667785\n",
      "Loss after iteration 11000: 0.21501590378154595\n",
      "Loss after iteration 12000: 0.20875374157004398\n",
      "Loss after iteration 13000: 0.20285871718765447\n",
      "Loss after iteration 14000: 0.19729411839320132\n",
      "Loss after iteration 15000: 0.19204852444466272\n",
      "Loss after iteration 16000: 0.1886673306666168\n",
      "Loss after iteration 17000: 0.18540482546620296\n",
      "Loss after iteration 18000: 0.18225239937784635\n",
      "Loss after iteration 19000: 0.17920372061281703\n",
      "Loss after iteration 20000: 0.17625292963731626\n",
      "Loss after iteration 21000: 0.17431280217869102\n",
      "Loss after iteration 22000: 0.17241413439860073\n",
      "Loss after iteration 23000: 0.17055441353735679\n",
      "Loss after iteration 24000: 0.1687322743519382\n",
      "Loss after iteration 25000: 0.16694733812228546\n",
      "Loss after iteration 26000: 0.16576205351223\n",
      "Loss after iteration 27000: 0.1645929575741289\n",
      "Loss after iteration 28000: 0.16343907945894007\n",
      "Loss after iteration 29000: 0.16230005537578396\n",
      "Loss after iteration 30000: 0.1611755321553214\n",
      "Loss after iteration 31000: 0.16042414390677392\n",
      "Loss after iteration 32000: 0.15967946345043044\n",
      "Loss after iteration 33000: 0.1589410208615475\n",
      "Loss after iteration 34000: 0.1582087111802316\n",
      "Loss after iteration 35000: 0.15748242358834183\n",
      "Loss after iteration 36000: 0.15699529159662018\n",
      "Loss after iteration 37000: 0.1565110458844752\n",
      "Loss after iteration 38000: 0.15602941042730284\n",
      "Loss after iteration 39000: 0.15555033959590497\n",
      "Loss after iteration 40000: 0.15507378212823894\n",
      "Loss after iteration 41000: 0.15475334290908674\n",
      "Loss after iteration 42000: 0.15443414414011455\n",
      "Loss after iteration 43000: 0.15411602906796212\n",
      "Loss after iteration 44000: 0.1537991803344617\n",
      "Loss after iteration 45000: 0.15348372569432917\n",
      "Loss after iteration 46000: 0.15327146714460887\n",
      "Loss after iteration 47000: 0.15305992721355097\n",
      "Loss after iteration 48000: 0.1528489951821517\n",
      "Loss after iteration 49000: 0.15263865883070393\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "x_train, y_train = generate_data()\n",
    "print(x_train.shape)\n",
    "loss_function = \"cross_entropy\"\n",
    "layers_dims = [1, 1000, 1]\n",
    "activation_fn = [\"relu\", \"sigmoid\"]\n",
    "learning_rate = 1e-5\n",
    "num_iterations = 50000\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 5000\n",
    "decrease_proportion = 0.675\n",
    "# You don't necessarily need to use mini_batch in this part\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQQtLyFgL4WD"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to:\n",
    "> * *Lab4_basic_regression.csv*\n",
    "> * *Lab4_basic_regression.jpg*\n",
    "> * *Lab4_basic_regression.gif*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "0uwle3uqL9Em"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_basic_regression.csv'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved as 'Lab4_basic_regression.jpg'\n",
      "Animation saved as 'Lab4_basic_regression.gif'\n"
     ]
    }
   ],
   "source": [
    "save_final_result(model, x_train, y_train)\n",
    "animate_training(history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX-kDur7xKOW"
   },
   "source": [
    "## Part 3: Binary classification (10%)\n",
    "\n",
    "You will train a model to perform binary classification. Your task is to predict whether an optical coherence tomography (OCT) image shows Choroidal Neovascularization (CNV) or is normal.\n",
    "\n",
    "- Data: OCT scan image of retina\n",
    "- Classes:\n",
    "  - CNV: label = 1\n",
    "  - Normal: label = 0\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 20000 images\n",
    "  - Testing set size: 5000 images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAvUwG1uSLg_"
   },
   "source": [
    "> ### Step 1: Read data & split data\n",
    "Load *basic_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hp8M93z7v_lO"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('basic_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names_binary = {0: 'Normal', 1: 'CNV'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(f'Label: {int(Y_train[i])} ({class_names_binary[int(Y_train[i])]})', fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = None\n",
    "X_test = None\n",
    "# Reshape Y_train to 2D array\n",
    "Y_train = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot data distribution\n",
    "Y_train_1 = np.sum(Y_train == 1)\n",
    "Y_train_0 = np.sum(Y_train == 0)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar([0, 1], [Y_train_0, Y_train_1])\n",
    "plt.title('Data distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print('Train: x=%s, y=%s' % (X_train.shape, Y_train.shape))\n",
    "print('Test: x=%s' % (X_test.shape, ))\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = None\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train = None\n",
    "y_train = None\n",
    "x_val = None\n",
    "y_val = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r01QzzHxeMbR"
   },
   "source": [
    "> ### Step 2: Training and Evaluation\n",
    "Train your model on the prepared OCT image data and evaluate its performance in distinguishing between CNV and normal retinal conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI7JY5ESjhZ2"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = None\n",
    "layers_dims = None\n",
    "activation_fn = None\n",
    "learning_rate = None\n",
    "num_iterations = None\n",
    "print_loss = True\n",
    "print_freq = 200\n",
    "decrease_freq = None\n",
    "decrease_proportion = None\n",
    "# You might need to use mini_batch to reduce training time in this part\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8q0a20XcPtk"
   },
   "outputs": [],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqtnepD-6I20"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_basic.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mERo3g41zsyX"
   },
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_test)),\n",
    "    'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_basic.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_basic.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMCpPFMVdj36"
   },
   "source": [
    "# **Advanced Part (30%)**\n",
    "\n",
    "You will train a model to perform multi-class classification on medical imaging data. Your task is to classify optical coherence tomography (OCT) images of retinal conditions into four different categories.\n",
    "\n",
    "- Data: OCT scan images of retina\n",
    "- Classes:\n",
    "  - CNV (Choroidal Neovascularization): label = 0\n",
    "  - DME (Diabetic Macular Edema): label = 1\n",
    "  - Drusen: label = 2\n",
    "  - Normal: label = 3\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 37754 images\n",
    "  - Testing set size: 6997 images\n",
    "\n",
    "**Notes:** You can implement other functions to improve your rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_GQ3uO128OC"
   },
   "source": [
    "## Step 1: Read data & split data\n",
    "\n",
    "Load *advanced_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVSfqnXqXGdC"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('advanced_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "print(f'Initial shapes:')\n",
    "print(f'Train: X={X_train.shape}, Y={Y_train.shape}')\n",
    "print(f'Test: X={X_test.shape}')\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names = {0: 'CNV', 1: 'DME', 2: 'Drusen', 3: 'Normal'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(f'Label: {int(Y_train[i])} ({class_names[int(Y_train[i])]})', fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 4  # OCT has 4 classes\n",
    "Y_train = None\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = None\n",
    "X_test = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of Y_train:\", Y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "\n",
    "# Plot class distribution before splitting\n",
    "orig_labels = np.argmax(Y_train, axis=1)\n",
    "unique, counts = np.unique(orig_labels, return_counts=True)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar(unique, counts)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = None\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train = None\n",
    "y_train = None\n",
    "x_val = None\n",
    "y_val = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngmUDGN13ADi"
   },
   "source": [
    "## Step 2: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIi1A-1dFY0u"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = None\n",
    "layers_dims = None\n",
    "activation_fn = None\n",
    "learning_rate = None\n",
    "num_iterations = None\n",
    "print_loss = True\n",
    "print_freq = 50\n",
    "decrease_freq = None\n",
    "decrease_proportion = None\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehjcfSU2XD3-"
   },
   "outputs": [],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXGnS3HQeNUc"
   },
   "source": [
    "## Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_advanced.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHFDuq2BQ2qI"
   },
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_test)),\n",
    "    'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_advanced.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_advanced.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J91ff4Vk1oB_"
   },
   "source": [
    "# Save outputs\n",
    "Save the outputs of your testing codes to: *Lab4_output.npy*\n",
    "\n",
    "We will test your *Lab4_output.npy* to verify the correctness of your neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpxmIFiW1tg9"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert list(outputs.keys()) == [\n",
    "    'dense_forward',\n",
    "    'dense_backward',\n",
    "    'dense_update_parameters',\n",
    "    'sigmoid',\n",
    "    'relu',\n",
    "    'softmax',\n",
    "    'linear',\n",
    "    'sigmoid_backward',\n",
    "    'relu_backward',\n",
    "    'softmax_backward',\n",
    "    'linear_backward',\n",
    "    'model_forward_sigmoid',\n",
    "    'model_forward_relu',\n",
    "    'model_forward_softmax',\n",
    "    'model_backward_sigmoid',\n",
    "    'model_backward_relu',\n",
    "    'model_update_parameters',\n",
    "    'compute_BCE_loss',\n",
    "    'compute_CCE_loss'\n",
    "], \"You're missing something, please restart the kernel and run the code from beginning to the end. If the same error occurs, maybe you deleted some outputs, check the template to find the missing parts!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDqCzhsp1yTb"
   },
   "outputs": [],
   "source": [
    "np.save(\"Lab4_output.npy\", outputs)\n",
    "\n",
    "# sanity check for saved outputs\n",
    "submit = np.load(\"Lab4_output.npy\", allow_pickle=True).item()\n",
    "for key, value in submit.items():\n",
    "    print(f\"{key}: {type(value)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
