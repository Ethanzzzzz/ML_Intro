{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHYT0wwCWjlN"
      },
      "source": [
        "# **Lab 2 : Decision Tree and Random Forest**\n",
        "In *lab 2*, you need to finish :\n",
        "\n",
        "1. Basic Part :\n",
        "  Implement a Decision Tree model and predict whether patients in the validation set survived.\n",
        "\n",
        "  > * Section 1: Function Implementation and Testing\n",
        "  > * Section 2: Building the Decision Tree Model\n",
        "\n",
        "\n",
        "2. Advanced Part : Build a **Random Forest** model to make predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtkN1RNQKznC"
      },
      "source": [
        "❗ **Important** ❗\n",
        "Please follow the template. Follow the instructions.\n",
        "**Do not** change the code outside this code bracket if you see one.\n",
        "```\n",
        "### START CODE HERE ###\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViMa9pPp0L8U"
      },
      "source": [
        "We'll be using **pandas** frequently in this template, so we've provided a link to help you get familiar with its usage: https://pandas.pydata.org/docs/user_guide/10min.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzqwVcaWqll"
      },
      "source": [
        "## Import Packages\n",
        "\n",
        "> Note : You **cannot** import any other packages in both basic and advanced part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "eb6ccSWDWrTd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwMS_7dWtwj"
      },
      "source": [
        "# **Basic Part** (30%)\n",
        "\n",
        "## Section 1: Function Implementation and Testing\n",
        "You will implement five functions that are necessary for building a decision tree model. After implementing each function, you must run it with the given input variables to verify its correctness. Save the results of each function to a CSV file for submission.\n",
        "> * Step 1: Calculate the Entropy\n",
        "> * Step 2: Calculate the Information Gain\n",
        "> * Step 3: Find the Best Split\n",
        "> * Step 4: Split the data into two branches\n",
        "> * Step 5: Build the decision tree\n",
        "> * Step 6: Save answers\n",
        "\n",
        "\n",
        "## Section 2: Build a Decision Tree Model and make Predictions\n",
        "After implementing the functions, you will use them to build a decision tree model and make predictions. Follow the steps below to train your model and evaluate its performance.\n",
        "> * Step 1: Split the data into training set and validation set\n",
        "> * Step 2: Train a decision tree model with the training set\n",
        "> * Step 3: Predict the cases in the validation set by using the model trained in Step 2\n",
        "> * Step 4: Calculate the f1-score of your predictions in Step 3\n",
        "> * Step 5: Save answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeEPi9tfWzx_"
      },
      "source": [
        "## Load the input data\n",
        "Let's load the input file **lab2_basic_input.csv**.\n",
        "\n",
        "> Note: you will use this input data in both section 1 and section 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "TIOE-YsHW3lA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>pre_icu_los_days</th>\n",
              "      <th>glucose_apache</th>\n",
              "      <th>heart_rate_apache</th>\n",
              "      <th>resprate_apache</th>\n",
              "      <th>sodium_apache</th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.0</td>\n",
              "      <td>26.596278</td>\n",
              "      <td>1</td>\n",
              "      <td>173.0</td>\n",
              "      <td>79.60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>199.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51.0</td>\n",
              "      <td>36.267895</td>\n",
              "      <td>0</td>\n",
              "      <td>180.3</td>\n",
              "      <td>117.90</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>88.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>81.0</td>\n",
              "      <td>24.196007</td>\n",
              "      <td>1</td>\n",
              "      <td>162.0</td>\n",
              "      <td>63.50</td>\n",
              "      <td>1.988194</td>\n",
              "      <td>285.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83.0</td>\n",
              "      <td>21.105377</td>\n",
              "      <td>1</td>\n",
              "      <td>162.6</td>\n",
              "      <td>55.80</td>\n",
              "      <td>0.211111</td>\n",
              "      <td>189.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>76.0</td>\n",
              "      <td>20.470093</td>\n",
              "      <td>0</td>\n",
              "      <td>167.6</td>\n",
              "      <td>57.50</td>\n",
              "      <td>14.493056</td>\n",
              "      <td>278.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60.0</td>\n",
              "      <td>46.111111</td>\n",
              "      <td>0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>149.40</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>186.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>70.0</td>\n",
              "      <td>17.361111</td>\n",
              "      <td>1</td>\n",
              "      <td>168.0</td>\n",
              "      <td>49.00</td>\n",
              "      <td>0.156944</td>\n",
              "      <td>181.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>79.0</td>\n",
              "      <td>33.274623</td>\n",
              "      <td>0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>90.70</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>56.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>81.0</td>\n",
              "      <td>30.462306</td>\n",
              "      <td>0</td>\n",
              "      <td>177.8</td>\n",
              "      <td>96.30</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>113.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>54.0</td>\n",
              "      <td>25.843929</td>\n",
              "      <td>0</td>\n",
              "      <td>177.8</td>\n",
              "      <td>81.70</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>112.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>54.0</td>\n",
              "      <td>35.008738</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>84.00</td>\n",
              "      <td>0.702083</td>\n",
              "      <td>89.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>55.0</td>\n",
              "      <td>28.697484</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>96.00</td>\n",
              "      <td>0.048611</td>\n",
              "      <td>88.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>65.0</td>\n",
              "      <td>15.741828</td>\n",
              "      <td>1</td>\n",
              "      <td>157.4</td>\n",
              "      <td>39.00</td>\n",
              "      <td>0.099306</td>\n",
              "      <td>92.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>68.0</td>\n",
              "      <td>27.771653</td>\n",
              "      <td>1</td>\n",
              "      <td>165.1</td>\n",
              "      <td>75.70</td>\n",
              "      <td>0.015278</td>\n",
              "      <td>211.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>81.0</td>\n",
              "      <td>21.575208</td>\n",
              "      <td>1</td>\n",
              "      <td>157.5</td>\n",
              "      <td>53.52</td>\n",
              "      <td>3.043056</td>\n",
              "      <td>90.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>67.0</td>\n",
              "      <td>30.506023</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>102.05</td>\n",
              "      <td>0.350694</td>\n",
              "      <td>107.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>77.0</td>\n",
              "      <td>25.827736</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>86.40</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>97.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>69.0</td>\n",
              "      <td>23.397612</td>\n",
              "      <td>1</td>\n",
              "      <td>165.0</td>\n",
              "      <td>63.70</td>\n",
              "      <td>0.035417</td>\n",
              "      <td>224.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>77.0</td>\n",
              "      <td>34.532872</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>99.80</td>\n",
              "      <td>0.035417</td>\n",
              "      <td>250.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>64.0</td>\n",
              "      <td>27.394313</td>\n",
              "      <td>1</td>\n",
              "      <td>167.0</td>\n",
              "      <td>76.40</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>373.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>51.0</td>\n",
              "      <td>28.219692</td>\n",
              "      <td>0</td>\n",
              "      <td>185.4</td>\n",
              "      <td>97.00</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>90.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>62.0</td>\n",
              "      <td>23.640816</td>\n",
              "      <td>0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>72.40</td>\n",
              "      <td>6.063889</td>\n",
              "      <td>202.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>54.0</td>\n",
              "      <td>34.710158</td>\n",
              "      <td>1</td>\n",
              "      <td>167.6</td>\n",
              "      <td>97.50</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>205.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>74.0</td>\n",
              "      <td>33.651380</td>\n",
              "      <td>0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>98.40</td>\n",
              "      <td>1.059722</td>\n",
              "      <td>184.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>52.0</td>\n",
              "      <td>20.402893</td>\n",
              "      <td>0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>63.20</td>\n",
              "      <td>13.668750</td>\n",
              "      <td>105.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>70.0</td>\n",
              "      <td>28.686787</td>\n",
              "      <td>1</td>\n",
              "      <td>170.2</td>\n",
              "      <td>83.10</td>\n",
              "      <td>0.167361</td>\n",
              "      <td>201.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>78.0</td>\n",
              "      <td>21.250000</td>\n",
              "      <td>1</td>\n",
              "      <td>160.0</td>\n",
              "      <td>54.40</td>\n",
              "      <td>0.271528</td>\n",
              "      <td>201.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>86.0</td>\n",
              "      <td>19.188698</td>\n",
              "      <td>0</td>\n",
              "      <td>185.6</td>\n",
              "      <td>66.10</td>\n",
              "      <td>0.786111</td>\n",
              "      <td>111.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>58.0</td>\n",
              "      <td>23.295905</td>\n",
              "      <td>1</td>\n",
              "      <td>165.1</td>\n",
              "      <td>63.50</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>264.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>69.0</td>\n",
              "      <td>51.288336</td>\n",
              "      <td>1</td>\n",
              "      <td>162.6</td>\n",
              "      <td>135.60</td>\n",
              "      <td>7.174306</td>\n",
              "      <td>158.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>78.0</td>\n",
              "      <td>30.524099</td>\n",
              "      <td>0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.173611</td>\n",
              "      <td>230.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>46.0</td>\n",
              "      <td>47.477519</td>\n",
              "      <td>1</td>\n",
              "      <td>162.0</td>\n",
              "      <td>124.60</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>598.7</td>\n",
              "      <td>39.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>63.0</td>\n",
              "      <td>28.138070</td>\n",
              "      <td>1</td>\n",
              "      <td>157.5</td>\n",
              "      <td>69.80</td>\n",
              "      <td>0.809028</td>\n",
              "      <td>105.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32.0</td>\n",
              "      <td>20.012958</td>\n",
              "      <td>0</td>\n",
              "      <td>175.3</td>\n",
              "      <td>61.50</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>573.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>76.0</td>\n",
              "      <td>31.265432</td>\n",
              "      <td>0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>101.30</td>\n",
              "      <td>0.077778</td>\n",
              "      <td>102.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>72.0</td>\n",
              "      <td>21.165166</td>\n",
              "      <td>1</td>\n",
              "      <td>152.0</td>\n",
              "      <td>48.90</td>\n",
              "      <td>0.268056</td>\n",
              "      <td>103.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43.0</td>\n",
              "      <td>31.294766</td>\n",
              "      <td>1</td>\n",
              "      <td>165.0</td>\n",
              "      <td>85.20</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>93.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>57.0</td>\n",
              "      <td>31.757926</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>76.20</td>\n",
              "      <td>2.721528</td>\n",
              "      <td>181.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>31.0</td>\n",
              "      <td>37.300976</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>89.50</td>\n",
              "      <td>0.423611</td>\n",
              "      <td>75.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>81.0</td>\n",
              "      <td>20.989855</td>\n",
              "      <td>0</td>\n",
              "      <td>167.6</td>\n",
              "      <td>58.96</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>212.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age        bmi  gender  height  weight  pre_icu_los_days  glucose_apache  \\\n",
              "0   28.0  26.596278       1   173.0   79.60          0.000000           199.0   \n",
              "1   51.0  36.267895       0   180.3  117.90          0.141667            88.0   \n",
              "2   81.0  24.196007       1   162.0   63.50          1.988194           285.0   \n",
              "3   83.0  21.105377       1   162.6   55.80          0.211111           189.0   \n",
              "4   76.0  20.470093       0   167.6   57.50         14.493056           278.0   \n",
              "5   60.0  46.111111       0   180.0  149.40          0.027778           186.0   \n",
              "6   70.0  17.361111       1   168.0   49.00          0.156944           181.0   \n",
              "7   79.0  33.274623       0   165.1   90.70          0.004861            56.0   \n",
              "8   81.0  30.462306       0   177.8   96.30          0.002083           113.0   \n",
              "9   54.0  25.843929       0   177.8   81.70          0.007639           112.0   \n",
              "10  54.0  35.008738       1   154.9   84.00          0.702083            89.0   \n",
              "11  55.0  28.697484       0   182.9   96.00          0.048611            88.0   \n",
              "12  65.0  15.741828       1   157.4   39.00          0.099306            92.0   \n",
              "13  68.0  27.771653       1   165.1   75.70          0.015278           211.0   \n",
              "14  81.0  21.575208       1   157.5   53.52          3.043056            90.0   \n",
              "15  67.0  30.506023       0   182.9  102.05          0.350694           107.0   \n",
              "16  77.0  25.827736       0   182.9   86.40          0.712500            97.0   \n",
              "17  69.0  23.397612       1   165.0   63.70          0.035417           224.0   \n",
              "18  77.0  34.532872       1   170.0   99.80          0.035417           250.0   \n",
              "19  64.0  27.394313       1   167.0   76.40          0.011111           373.0   \n",
              "20  51.0  28.219692       0   185.4   97.00          0.537500            90.0   \n",
              "21  62.0  23.640816       0   175.0   72.40          6.063889           202.0   \n",
              "22  54.0  34.710158       1   167.6   97.50          0.044444           205.0   \n",
              "23  74.0  33.651380       0   171.0   98.40          1.059722           184.0   \n",
              "24  52.0  20.402893       0   176.0   63.20         13.668750           105.0   \n",
              "25  70.0  28.686787       1   170.2   83.10          0.167361           201.0   \n",
              "26  78.0  21.250000       1   160.0   54.40          0.271528           201.0   \n",
              "27  86.0  19.188698       0   185.6   66.10          0.786111           111.0   \n",
              "28  58.0  23.295905       1   165.1   63.50          0.001389           264.0   \n",
              "29  69.0  51.288336       1   162.6  135.60          7.174306           158.0   \n",
              "30  78.0  30.524099       0   181.0  100.00          0.173611           230.0   \n",
              "31  46.0  47.477519       1   162.0  124.60          0.000694           598.7   \n",
              "32  63.0  28.138070       1   157.5   69.80          0.809028           105.0   \n",
              "33  32.0  20.012958       0   175.3   61.50          0.004861           573.0   \n",
              "34  76.0  31.265432       0   180.0  101.30          0.077778           102.0   \n",
              "35  72.0  21.165166       1   152.0   48.90          0.268056           103.0   \n",
              "36  43.0  31.294766       1   165.0   85.20          0.083333            93.0   \n",
              "37  57.0  31.757926       1   154.9   76.20          2.721528           181.0   \n",
              "38  31.0  37.300976       1   154.9   89.50          0.423611            75.0   \n",
              "39  81.0  20.989855       0   167.6   58.96          0.131250           212.0   \n",
              "\n",
              "    heart_rate_apache  resprate_apache  sodium_apache  hospital_death  \n",
              "0                52.0             29.0          140.0               0  \n",
              "1               104.0             31.0          143.0               0  \n",
              "2               178.0              4.0          138.0               1  \n",
              "3               115.0             18.0          158.0               0  \n",
              "4                93.0              8.0          134.0               1  \n",
              "5               146.0             34.0          139.0               1  \n",
              "6               111.0             12.0          158.0               1  \n",
              "7                37.0             44.0          141.0               0  \n",
              "8                62.0              4.0          142.0               0  \n",
              "9               110.0             24.0          136.0               1  \n",
              "10              105.0             37.0          144.0               0  \n",
              "11              115.0             33.0          138.0               0  \n",
              "12               58.0              8.0          121.0               0  \n",
              "13              104.0              4.0          128.0               1  \n",
              "14               99.0              8.0          136.0               0  \n",
              "15              105.0             11.0          149.0               1  \n",
              "16               92.0             18.0          139.0               0  \n",
              "17               90.0             37.0          137.0               1  \n",
              "18              116.0             11.0          141.0               1  \n",
              "19              141.0             31.0          139.0               1  \n",
              "20              146.0             45.0          132.0               1  \n",
              "21               52.0             12.0          129.0               1  \n",
              "22              110.0             52.0          137.0               0  \n",
              "23              103.0              4.0          136.0               0  \n",
              "24              133.0             44.0          135.0               1  \n",
              "25               95.0             25.0          139.0               1  \n",
              "26              134.0             60.0          130.0               0  \n",
              "27              152.0             30.0          137.0               1  \n",
              "28              178.0             40.0          139.0               1  \n",
              "29              154.0             45.0          138.0               1  \n",
              "30               98.0              9.0          132.0               1  \n",
              "31               39.0             12.0          130.0               1  \n",
              "32               99.0              6.0          138.0               0  \n",
              "33              107.0              7.0          140.0               0  \n",
              "34              124.0              6.0          150.0               1  \n",
              "35              130.0             31.0          147.0               1  \n",
              "36              122.0             10.0          144.0               0  \n",
              "37              137.0             47.0          141.0               1  \n",
              "38              116.0             42.0          139.0               0  \n",
              "39              138.0             12.0          154.0               0  "
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data = pd.read_csv('lab2_basic_input.csv')\n",
        "input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cdYaIEjW7-r"
      },
      "source": [
        "## Global attributes\n",
        "Define the global attributes\n",
        "> Note : You **cannot** modify the values of these attributes we have provided in the basic part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "S4WLhABvW6Qr"
      },
      "outputs": [],
      "source": [
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = input_data.shape[1] - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxs3U-K2W-ZI"
      },
      "source": [
        "> You can add your own global attributes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qvPkvOpXBgX"
      },
      "source": [
        "## Section 1: Function Implementation and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WWTmDqyXGR5"
      },
      "source": [
        "### Step 1 & 2: Calculate the Entropy and Information Gain\n",
        "In these steps, you will implement functions to calculate entropy and information gain. These metrics are crucial for determining the best way to split the dataset at each node in the decision tree.\n",
        "\n",
        "If you need some help on Entropy and Information Gain, please refer to\n",
        "* https://codingnomads.com/decision-tree-information-gain-entropy#what-is-entropy\n",
        "* https://www.mldawn.com/decision-trees-entropy/#:~:text=In%20a%20binary%20classification%20problem%2C%20when%20Entropy%20hits%200%20it,state%20of%20purity%20and%20certainty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "4j-LodsRXLd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_entropy =  0.9928\n"
          ]
        }
      ],
      "source": [
        "def entropy(data):\n",
        "  \"\"\"\n",
        "  This function measures the amount of uncertainty in a probability distribution\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you're calculating for the entropy\n",
        "  return:\n",
        "  * entropy_value(type: float): the data's entropy\n",
        "  \"\"\"\n",
        "  p = 0 # to count the number of cases that survived\n",
        "  n = 0 # to count the number of cases that passed away\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Hint 1: what is the equation for calculating entropy?\n",
        "  # Hint 2: consider the case when p == 0 or n == 0, what should entropy be?\n",
        "\n",
        "  for death in data['hospital_death']:\n",
        "    if death:\n",
        "      n += 1\n",
        "    else:\n",
        "      p += 1\n",
        "\n",
        "  entropy_value = - (p/(p+n)) * math.log2(p/(p+n)) - (n/(p+n)) * math.log2(n/(p+n)) if p != 0 and n != 0 else 0\n",
        "  entropy_value = round(entropy_value, 4)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return entropy_value\n",
        "\n",
        "# [Note] You have to save the value of \"ans_entropy\" into the output file\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_entropy = entropy(input_data)\n",
        "print(\"ans_entropy = \", ans_entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6pZ-HjJiDWv"
      },
      "source": [
        "Expected output:\n",
        "> ans_entropy =  0.9928"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "hTvkGOCdXS0H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_informationGain =  0.0385\n"
          ]
        }
      ],
      "source": [
        "def information_gain(data, mask):\n",
        "  \"\"\"\n",
        "  This function will calculate the information gain\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you're calculating for the information gain\n",
        "  * mask(type: Series): partition information(left/right) of current input data,\n",
        "    - boolean 1(True) represents split to left subtree\n",
        "    - boolean 0(False) represents split to right subtree\n",
        "  return:\n",
        "  * ig(type: float): the information gain you can obtain by classifying the data with this given mask\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  # Hint: you should use mask to split the data into two, then recall what is the equation for calculating information gain\n",
        "  left = data[mask]\n",
        "  right = data[~mask]\n",
        "\n",
        "  # print(data)\n",
        "  # print(\"------\")\n",
        "  # print(left)\n",
        "  sizeL = left.shape[0]\n",
        "  sizeR = right.shape[0]\n",
        "  sizeT = data.shape[0]\n",
        "\n",
        "  ig = entropy(data) - ((sizeL/sizeT) * entropy(left) + (sizeR/sizeT) * entropy(right))\n",
        "  ig = round(ig, 4)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return ig\n",
        "\n",
        "# [Note] You have to save the value of \"ans_informationGain\" into your output file\n",
        "# Here, let's assume that we split the input_data with 2/3 of the data in the left subtree and 1/3 in the right subtree\n",
        "# Please round your answer to 4 decimal place\n",
        "temp1 = np.zeros((int(input_data.shape[0]/3), 1), dtype=bool)\n",
        "temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/3), 1)), dtype=bool)\n",
        "temp_mask = np.concatenate((temp1, temp2))\n",
        "df_mask = pd.DataFrame(temp_mask, columns=['mask'])\n",
        "ans_informationGain = information_gain(input_data, df_mask['mask'])\n",
        "print(\"ans_informationGain = \", ans_informationGain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5RLtCwcieQ8"
      },
      "source": [
        "Expected output:\n",
        "> ans_informationGain = 0.0385"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXajOk9jXefG"
      },
      "source": [
        "### Step 3: Find the Best Split\n",
        "In this step, you will use the information gain calculated for each feature to find the best split. The best split is the point where the dataset is divided into two subgroups (left and right subtrees) in a way that maximizes the reduction of entropy.\n",
        "\n",
        "\n",
        "> Method: The process involves evaluating **every possible split** for each feature in the dataset. After sorting the data, you calculate potential split points by taking the **median of two consecutive values** where they differ. This median value becomes the threshold for splitting the data into two branches. The split that results in the highest information gain is selected as the best split.\n",
        "\n",
        "> Note: The method we have provided is a straightforward and basic approach. Please use this method to complete the basic part of the assignment. However, for the advanced part, you are welcome to explore and use other methods to fine-tune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "oT8UOEtzXiEu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_ig =  0.2146\n",
            "ans_value =  99.5\n",
            "ans_name =  glucose_apache\n"
          ]
        }
      ],
      "source": [
        "def find_best_split(data, impl_part):\n",
        "  \"\"\"\n",
        "  This function will find the best split combination of data\n",
        "  args:\n",
        "  * data(type: DataFrame): the input data\n",
        "  * impl_part(type: string): 'basic' or 'advanced' to specify which implementation to use\n",
        "  return\n",
        "  * best_ig(type: float): the best information gain you obtain\n",
        "  * best_threshold(type: float): the value that splits data into 2 branches\n",
        "  * best_feature(type: string): the feature that splits data into 2 branches\n",
        "  \"\"\"\n",
        "  best_ig = -1e9\n",
        "  best_threshold = 0\n",
        "  best_feature = ''\n",
        "\n",
        "  if(impl_part == 'basic'):\n",
        "    # Implement this part of the function using the method we provided\n",
        "    ### START CODE HERE ###\n",
        "    for f in data:\n",
        "      if f == 'hospital_death':\n",
        "        continue\n",
        "      sorted_data = data.sort_values(by=f)\n",
        "      for idx in range(sorted_data[f].shape[0]-1):\n",
        "        threshold = (sorted_data[f].iloc[idx] + sorted_data[f].iloc[idx+1]) / 2\n",
        "        mask = sorted_data[f] < threshold\n",
        "        ig = information_gain(sorted_data, mask)\n",
        "        if ig > best_ig:\n",
        "          best_ig = ig\n",
        "          best_threshold = float(threshold)\n",
        "          best_feature = f\n",
        "    ### END CODE HERE ###\n",
        "  else:\n",
        "    # You can implement another method here for the advanced part\n",
        "    ### START CODE HERE ###\n",
        "    pass\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "  return best_ig, best_threshold, best_feature\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n",
        "# Here, let's try to find the best split for the input_data\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_ig, ans_value, ans_name = find_best_split(input_data, 'basic')\n",
        "print(\"ans_ig = \", ans_ig)\n",
        "print(\"ans_value = \", ans_value)\n",
        "print(\"ans_name = \", ans_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovN6pxbXjQE6"
      },
      "source": [
        "Expected output:\n",
        "> ans_ig =  0.2146\n",
        "\n",
        "> ans_value =  99.5\n",
        "\n",
        "> ans_name =  glucose_apache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6koURsiXwjm"
      },
      "source": [
        "### Step 4: Split into 2 branches\n",
        "\n",
        "When you are building a decision tree, after identifying the best split, you will divide the dataset into two branches: a left branch and a right branch. Each branch represents a subset of the data based on the chosen **feature** and **split point**.\n",
        "\n",
        "* The left branch will contain the data points that meet the condition of the split (e.g., values less than or equal to the split threshold).\n",
        "* The right branch will contain the remaining data points (e.g., values greater than the split threshold).\n",
        "\n",
        "This step is essential because it creates the subgroups that the decision tree will continue to split in subsequent steps. By repeatedly splitting the data into smaller and more homogenous branches, the tree becomes more capable of accurately classifying new data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "0u0_dlPwX07H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_left =  7\n"
          ]
        }
      ],
      "source": [
        "def make_partition(data, feature, threshold):\n",
        "  \"\"\"\n",
        "  This function will split the data into 2 branches\n",
        "  args:\n",
        "  * data(type: DataFrame): the input data\n",
        "  * feature(type: string): the attribute(column name)\n",
        "  * threshold(type: float): the threshold for splitting the data\n",
        "  return:\n",
        "  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n",
        "  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  left = data[data[feature] <= threshold]\n",
        "  right = data[data[feature] > threshold]\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return left, right\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_left\" into the output file\n",
        "# Here, let's assume the best split is when we choose bmi as the feature and threshold as 21.0\n",
        "left, right = make_partition(input_data, 'bmi', 21.0)\n",
        "ans_left = left.shape[0]\n",
        "print(\"ans_left = \", ans_left)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJjyqmgXj2GX"
      },
      "source": [
        "Expected output:\n",
        "> ans_left = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKThK7yX8XX"
      },
      "source": [
        "### Step 5: Build the Decision Tree\n",
        "Hang in there... we are almost done with this section!\n",
        "\n",
        "Now, you need to use the above functions to complete a build_tree function.\n",
        "\n",
        "> Method:\n",
        "1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n",
        "2.  Use function *find_best_split()* to find the best split combination\n",
        "3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n",
        "4. Use function *make_partition()* to split the data into two parts\n",
        "5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "ZC7qkOjAYAlQ"
      },
      "outputs": [],
      "source": [
        "def build_tree(data, max_depth, min_samples_split, depth):\n",
        "  \"\"\"\n",
        "  This function will build the decision tree\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you want to apply to the decision tree\n",
        "  * max_depth: the maximum depth of a decision tree\n",
        "  * min_samples_split: the minimum number of instances required to do partition\n",
        "  * depth: the height of the current decision tree\n",
        "  return:\n",
        "  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  global ans_features, ans_thresholds\n",
        "  # check the condition of current depth and the remaining number of samples\n",
        "  if depth < max_depth and data.shape[0] > min_samples_split :\n",
        "    # call find_best_split() to find the best combination\n",
        "    ig, threshold, feature = find_best_split(data, 'basic')\n",
        "    # check the value of information gain is greater than 0 or not\n",
        "    if ig > 0:\n",
        "      # update the depth\n",
        "      depth += 1\n",
        "      # call make_partition() to split the data into two parts\n",
        "      left, right = make_partition(data, feature, threshold)\n",
        "      # If there is no data split to the left tree OR no data split to the right tree\n",
        "      if left.shape[0] == 0 or right.shape[0] == 0 :\n",
        "        # return the label of the majority\n",
        "        label = int(data['hospital_death'].mode()[0])\n",
        "        return label\n",
        "      else:\n",
        "        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n",
        "        subtree = {question: []}\n",
        "\n",
        "        # call function build_tree() to recursively build the left subtree and right subtree\n",
        "        left_subtree = build_tree(left, max_depth, min_samples_split, depth)\n",
        "        right_subtree = build_tree(right, max_depth, min_samples_split, depth)\n",
        "        if left_subtree == right_subtree:\n",
        "          subtree = left_subtree\n",
        "        else:\n",
        "          subtree[question].append(left_subtree)\n",
        "          subtree[question].append(right_subtree)\n",
        "    else:\n",
        "      # return the label of the majority\n",
        "      label = int(data['hospital_death'].mode()[0])\n",
        "      return label\n",
        "  else:\n",
        "    # return the label of the majority\n",
        "    label = int(data['hospital_death'].mode()[0])\n",
        "    return label\n",
        "  # save the features and the corresponding thresholds into ans_features[] and ans_thresholds[]\n",
        "  if type(subtree) == dict:\n",
        "    arr = list(subtree.keys())[0].split()\n",
        "    ans_features.append(arr[0])\n",
        "    ans_thresholds.append(arr[2])\n",
        "  ### END CODE HERE ###\n",
        "  return subtree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7URY3IMkULl"
      },
      "source": [
        "An example of the output from *build_tree()*\n",
        "```\n",
        "{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n",
        "```\n",
        "Therefore,\n",
        "```\n",
        "ans_features = ['bmi', 'age']\n",
        "ans_thresholds = [33.5, 68.5]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "dAuaqjhuYQSi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'glucose_apache <= 99.5': [{'height <= 184.15': [0, 1]}, 1]}"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here, let's build a decision tree using the input_data\n",
        "\n",
        "ans_features = []\n",
        "ans_thresholds = []\n",
        "\n",
        "decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n",
        "decisionTree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD6A_jADyTjw"
      },
      "source": [
        "Expected output:\n",
        "> decisionTree = {'glucose_apache <= 99.5': [{'height <= 184.15': [0, 1]}, 1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "oTdQ3vVkYYEQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['height', 'glucose_apache']"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [Note] You have to save the features in the \"decisionTree\" structure into the output file\n",
        "ans_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZO17E9vkbi4"
      },
      "source": [
        "Expected output:\n",
        "> ans_features = ['height', 'glucose_apache']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "BUeZh1o5YYem"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['184.15', '99.5']"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n",
        "ans_thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUPTy5RWj-1D"
      },
      "source": [
        "Expected output:\n",
        "> ans_thresholds = [184.15, 99.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69TQQNVaYdmp"
      },
      "source": [
        "### Step 6: Save answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "B9DosXRQYbDg"
      },
      "outputs": [],
      "source": [
        "basic = []\n",
        "basic.append(ans_entropy)\n",
        "basic.append(ans_informationGain)\n",
        "basic.append([ans_ig, ans_value, ans_name])\n",
        "basic.append(ans_left)\n",
        "basic.append(ans_features + ans_thresholds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpMpPwySYlUu"
      },
      "source": [
        "## Section 2: Build a Decision Tree Model\n",
        "\n",
        "Congrats! You have completed all 5 crucial functions. Now, we will use the functions above to implement a simple decision tree. You will train the decision tree using a training set and make predictions using a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1egGj-lYn6O"
      },
      "source": [
        "### Step 1: Split data into training set and validation set\n",
        "> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "GAHFMO4QYpZ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40, 11)\n",
            "(30, 11)\n",
            "(10, 11)\n"
          ]
        }
      ],
      "source": [
        "num_train = 30\n",
        "num_validation = 10\n",
        "\n",
        "training_data = input_data.iloc[:num_train]\n",
        "validation_data = input_data.iloc[-num_validation:]\n",
        "\n",
        "y_train = training_data[['hospital_death']]\n",
        "x_train = training_data.drop(['hospital_death'], axis=1)\n",
        "\n",
        "y_validation = validation_data[['hospital_death']]\n",
        "x_validation = validation_data.drop(['hospital_death'], axis=1)\n",
        "y_validation = y_validation.values.flatten()\n",
        "\n",
        "print(input_data.shape)\n",
        "print(training_data.shape)\n",
        "print(validation_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQlhfOJ0YySZ"
      },
      "source": [
        "### Step 2 to 4 : Make predictions with a decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5IHxhEjY1rN"
      },
      "source": [
        "Define the attributions of the decision tree\n",
        "> You **cannot** modify the values of these attributes in this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Vwt7BRJ1Y3hD"
      },
      "outputs": [],
      "source": [
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = x_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-ly7gvtY5VQ"
      },
      "source": [
        "We have finished the function 'classify_data()' below, however, you can modify this function if you prefer completing it on your own way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "V4LfxAnYY6UQ"
      },
      "outputs": [],
      "source": [
        "def classify_data(instance, tree):\n",
        "  \"\"\"\n",
        "  This function will predict/classify the input instance\n",
        "  args:\n",
        "  * instance: a instance(case) to be predicted\n",
        "  return:\n",
        "  * answer: the prediction result (the classification result)\n",
        "  \"\"\"\n",
        "  equation = list(tree.keys())[0]\n",
        "  if equation.split()[1] == '<=':\n",
        "    temp_feature = equation.split()[0]\n",
        "    temp_threshold = equation.split()[2]\n",
        "    if instance[temp_feature] > float(temp_threshold):\n",
        "      answer = tree[equation][1]\n",
        "    else:\n",
        "      answer = tree[equation][0]\n",
        "  else:\n",
        "    if instance[equation.split()[0]] in (equation.split()[2]):\n",
        "      answer = tree[equation][0]\n",
        "    else:\n",
        "      answer = tree[equation][1]\n",
        "\n",
        "  if not isinstance(answer, dict):\n",
        "    return answer\n",
        "  else:\n",
        "    return classify_data(instance, answer)\n",
        "\n",
        "\n",
        "def make_prediction(tree, data):\n",
        "  \"\"\"\n",
        "  This function will use your pre-trained decision tree to predict the labels of all instances in data\n",
        "  args:\n",
        "  * tree: the decision tree\n",
        "  * data: the data to predict\n",
        "  return:\n",
        "  * y_prediction: the predictions\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  # [Note] You can call the function classify_data() to predict the label of each instance\n",
        "  y_prediction = []\n",
        "  for i, r in data.iterrows():\n",
        "    y_prediction.append(classify_data(r, tree))\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return y_prediction\n",
        "\n",
        "\n",
        "def calculate_score(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  This function will calculate the f1-score of the predictions\n",
        "  args:\n",
        "  * y_true: the ground truth\n",
        "  * y_pred: the predictions\n",
        "  return:\n",
        "  * score: the f1-score\n",
        "  \"\"\"\n",
        "  score = f1_score(y_true, y_pred)\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "_VEmtbmtZLQJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_f1score =  0.4444\n"
          ]
        }
      ],
      "source": [
        "decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n",
        "\n",
        "y_pred = make_prediction(decision_tree, x_validation)\n",
        "\n",
        "# [Note] You have to save the value of \"ans_f1score\" into your output file\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_f1score = calculate_score(y_validation, y_pred)\n",
        "ans_f1score = round(ans_f1score, 4)\n",
        "print(\"ans_f1score = \", ans_f1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUcJZRYZk4kX"
      },
      "source": [
        "Expected output:\n",
        "> ans_f1score =  0.4444"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "5t58_-BwpGnY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is just for you to check your predictions\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7NM7CvdpHzb"
      },
      "source": [
        "Expected output:\n",
        "> y_pred = [1, 1, 0, 1, 0, 0, 0, 0, 0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COjs0B5jZQ8y"
      },
      "source": [
        "### Step 5: Save answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "Hijx-U2yZUAk"
      },
      "outputs": [],
      "source": [
        "basic.append(ans_f1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9lr2gbVZUwc"
      },
      "source": [
        "## Write to Output File\n",
        "Save all of your answers into a csv file named **lab2_basic.csv**\n",
        "> Note: Please do not touch the code in this step, we have made sure this outputs the correct file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "a5_ifgVZZZKZ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ans</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.2146, 99.5, glucose_apache]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[height, glucose_apache, 184.15, 99.5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.4444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Ans\n",
              "Id                                        \n",
              "0                                   0.9928\n",
              "1                                   0.0385\n",
              "2           [0.2146, 99.5, glucose_apache]\n",
              "3                                        7\n",
              "4   [height, glucose_apache, 184.15, 99.5]\n",
              "5                                   0.4444"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_path = 'lab2_basic.csv'\n",
        "\n",
        "basic_df = pd.DataFrame({'Id': range(len(basic)), 'Ans': basic})\n",
        "basic_df.set_index('Id', inplace=True)\n",
        "basic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "OTK0JT1965qS"
      },
      "outputs": [],
      "source": [
        "basic_df.to_csv(basic_path, header = True, index = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chm8Ro6zZsWp"
      },
      "source": [
        "# **Advanced Part** (65%)\n",
        "\n",
        "In the advanced section of this lab, you will enhance your prediction capabilities by implementing a more powerful and complex machine learning model—Random Forests. Random Forests are an ensemble learning method that builds multiple decision trees and combines their outputs to improve prediction accuracy and model robustness.\n",
        "> * Step 1: Load training and testing data\n",
        "> * Step 2: Split training data into training and validation set\n",
        "> * Step 3: Build a Random Forest\n",
        "> * Step 4: Make predictions with the random forest\n",
        "> * Step 5: Write the Output File\n",
        "\n",
        "> ❗ **Important** ❗ You are allowed to create new functions to fine tune your random forest, but please make sure to **complete the functions provided**.\n",
        "\n",
        "\n",
        "\n",
        "We have attached some references if you need help:\n",
        "> https://medium.com/chung-yi/ml%E5%85%A5%E9%96%80-%E5%8D%81%E4%B8%83-%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-6afc24871857\n",
        "\n",
        "> https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kBs4D02Z0Ub"
      },
      "source": [
        "### Step 1: Load training and testing data\n",
        "First, load **lab2_advanced_training.csv**. You will use this to **train** the random forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "vLTMFK14Z4z5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>pre_icu_los_days</th>\n",
              "      <th>arf_apache</th>\n",
              "      <th>bun_apache</th>\n",
              "      <th>creatinine_apache</th>\n",
              "      <th>gcs_eyes_apache</th>\n",
              "      <th>...</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>apache_4a_icu_death_prob</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>79.0</td>\n",
              "      <td>25.616497</td>\n",
              "      <td>1</td>\n",
              "      <td>168.0</td>\n",
              "      <td>72.3</td>\n",
              "      <td>0.305556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.92</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.20</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43.0</td>\n",
              "      <td>23.494409</td>\n",
              "      <td>0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>68.7</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>39.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.20</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62.0</td>\n",
              "      <td>29.145882</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>97.5</td>\n",
              "      <td>0.006250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>3.59</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.20</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>72.0</td>\n",
              "      <td>41.183318</td>\n",
              "      <td>1</td>\n",
              "      <td>170.2</td>\n",
              "      <td>119.3</td>\n",
              "      <td>1.945139</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>2.25</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.40</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>87.0</td>\n",
              "      <td>22.914211</td>\n",
              "      <td>0</td>\n",
              "      <td>170.1</td>\n",
              "      <td>66.3</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.60</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.50</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8495</th>\n",
              "      <td>55.0</td>\n",
              "      <td>33.201250</td>\n",
              "      <td>0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>90.5</td>\n",
              "      <td>0.052083</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.09</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.60</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8496</th>\n",
              "      <td>87.0</td>\n",
              "      <td>29.756001</td>\n",
              "      <td>1</td>\n",
              "      <td>142.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.014583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.80</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.70</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8497</th>\n",
              "      <td>80.0</td>\n",
              "      <td>17.630854</td>\n",
              "      <td>1</td>\n",
              "      <td>165.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.102778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.90</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.80</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8498</th>\n",
              "      <td>74.0</td>\n",
              "      <td>19.199423</td>\n",
              "      <td>0</td>\n",
              "      <td>175.3</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.460417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>2.19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>33.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8499</th>\n",
              "      <td>77.0</td>\n",
              "      <td>29.412958</td>\n",
              "      <td>1</td>\n",
              "      <td>157.0</td>\n",
              "      <td>72.5</td>\n",
              "      <td>0.538889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.94</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8500 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age        bmi  gender  height  weight  pre_icu_los_days  arf_apache  \\\n",
              "0     79.0  25.616497       1   168.0    72.3          0.305556         0.0   \n",
              "1     43.0  23.494409       0   171.0    68.7          0.011806         0.0   \n",
              "2     62.0  29.145882       0   182.9    97.5          0.006250         0.0   \n",
              "3     72.0  41.183318       1   170.2   119.3          1.945139         0.0   \n",
              "4     87.0  22.914211       0   170.1    66.3          0.085417         0.0   \n",
              "...    ...        ...     ...     ...     ...               ...         ...   \n",
              "8495  55.0  33.201250       0   165.1    90.5          0.052083         0.0   \n",
              "8496  87.0  29.756001       1   142.0    60.0          0.014583         0.0   \n",
              "8497  80.0  17.630854       1   165.0    48.0          0.102778         0.0   \n",
              "8498  74.0  19.199423       0   175.3    59.0          0.460417         0.0   \n",
              "8499  77.0  29.412958       1   157.0    72.5          0.538889         0.0   \n",
              "\n",
              "      bun_apache  creatinine_apache  gcs_eyes_apache  ...  temp_apache  \\\n",
              "0           20.0               0.92              4.0  ...         36.3   \n",
              "1            9.0               0.70              1.0  ...         39.5   \n",
              "2           54.0               3.59              1.0  ...         35.0   \n",
              "3           53.0               2.25              4.0  ...         37.1   \n",
              "4           33.0               1.60              4.0  ...         36.1   \n",
              "...          ...                ...              ...  ...          ...   \n",
              "8495        15.0               1.09              4.0  ...         37.7   \n",
              "8496        17.0               0.80              4.0  ...         35.7   \n",
              "8497        30.0               0.90              3.0  ...         35.6   \n",
              "8498        39.0               2.19              1.0  ...         33.7   \n",
              "8499        22.0               0.94              4.0  ...         36.3   \n",
              "\n",
              "      ventilated_apache  wbc_apache  apache_4a_hospital_death_prob  \\\n",
              "0                   1.0        7.20                           0.28   \n",
              "1                   1.0       21.20                           0.53   \n",
              "2                   1.0       19.20                           0.62   \n",
              "3                   0.0       10.40                           0.11   \n",
              "4                   1.0       16.50                           0.16   \n",
              "...                 ...         ...                            ...   \n",
              "8495                1.0        7.60                           0.06   \n",
              "8496                0.0       11.70                           0.05   \n",
              "8497                1.0       45.80                           0.25   \n",
              "8498                1.0        3.20                           0.79   \n",
              "8499                0.0        1.93                           0.17   \n",
              "\n",
              "      apache_4a_icu_death_prob  aids  cirrhosis  diabetes_mellitus  leukemia  \\\n",
              "0                         0.07   0.0        0.0                0.0       0.0   \n",
              "1                         0.48   0.0        0.0                0.0       0.0   \n",
              "2                         0.45   0.0        0.0                0.0       0.0   \n",
              "3                         0.02   0.0        0.0                1.0       0.0   \n",
              "4                         0.08   0.0        0.0                0.0       0.0   \n",
              "...                        ...   ...        ...                ...       ...   \n",
              "8495                      0.04   0.0        0.0                0.0       0.0   \n",
              "8496                      0.02   0.0        0.0                0.0       0.0   \n",
              "8497                      0.12   0.0        0.0                0.0       0.0   \n",
              "8498                      0.72   0.0        0.0                0.0       0.0   \n",
              "8499                      0.09   0.0        0.0                1.0       0.0   \n",
              "\n",
              "      hospital_death  \n",
              "0                  0  \n",
              "1                  0  \n",
              "2                  1  \n",
              "3                  1  \n",
              "4                  0  \n",
              "...              ...  \n",
              "8495               0  \n",
              "8496               0  \n",
              "8497               1  \n",
              "8498               1  \n",
              "8499               0  \n",
              "\n",
              "[8500 rows x 30 columns]"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_training_data = pd.read_csv('lab2_advanced_training.csv')\n",
        "advanced_training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjPSZoAuZ4Ry"
      },
      "source": [
        "Next, load **lab2_advanced_testing.csv**. You will make predictions on this testing data using the pre-trained random forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "6k-HFk7tZ_eN"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>pre_icu_los_days</th>\n",
              "      <th>arf_apache</th>\n",
              "      <th>bun_apache</th>\n",
              "      <th>creatinine_apache</th>\n",
              "      <th>gcs_eyes_apache</th>\n",
              "      <th>...</th>\n",
              "      <th>sodium_apache</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>apache_4a_icu_death_prob</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>leukemia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82</td>\n",
              "      <td>38.733847</td>\n",
              "      <td>1</td>\n",
              "      <td>158.23</td>\n",
              "      <td>96.82</td>\n",
              "      <td>0.232639</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>135</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>22.692476</td>\n",
              "      <td>0</td>\n",
              "      <td>173.67</td>\n",
              "      <td>69.40</td>\n",
              "      <td>0.121528</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>133</td>\n",
              "      <td>32.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>33.702285</td>\n",
              "      <td>0</td>\n",
              "      <td>177.47</td>\n",
              "      <td>105.70</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.71</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>143</td>\n",
              "      <td>33.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81</td>\n",
              "      <td>20.274075</td>\n",
              "      <td>0</td>\n",
              "      <td>171.74</td>\n",
              "      <td>61.10</td>\n",
              "      <td>0.664583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>2.09</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>136</td>\n",
              "      <td>36.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41</td>\n",
              "      <td>29.027749</td>\n",
              "      <td>1</td>\n",
              "      <td>175.75</td>\n",
              "      <td>90.00</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>149</td>\n",
              "      <td>32.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>61</td>\n",
              "      <td>22.840598</td>\n",
              "      <td>0</td>\n",
              "      <td>174.96</td>\n",
              "      <td>71.20</td>\n",
              "      <td>0.009722</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "      <td>0.90</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>134</td>\n",
              "      <td>36.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>74</td>\n",
              "      <td>28.843833</td>\n",
              "      <td>1</td>\n",
              "      <td>169.31</td>\n",
              "      <td>82.90</td>\n",
              "      <td>0.389583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "      <td>0.74</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>36.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>68</td>\n",
              "      <td>22.744572</td>\n",
              "      <td>1</td>\n",
              "      <td>170.02</td>\n",
              "      <td>67.05</td>\n",
              "      <td>2.172917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>1.87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>137</td>\n",
              "      <td>35.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>55</td>\n",
              "      <td>25.356784</td>\n",
              "      <td>0</td>\n",
              "      <td>169.90</td>\n",
              "      <td>73.90</td>\n",
              "      <td>4.186111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "      <td>1.38</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>139</td>\n",
              "      <td>35.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>62</td>\n",
              "      <td>16.720580</td>\n",
              "      <td>1</td>\n",
              "      <td>165.40</td>\n",
              "      <td>46.00</td>\n",
              "      <td>0.157639</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11</td>\n",
              "      <td>2.80</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>135</td>\n",
              "      <td>36.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age        bmi  gender  height  weight  pre_icu_los_days  arf_apache  \\\n",
              "0     82  38.733847       1  158.23   96.82          0.232639         0.0   \n",
              "1     65  22.692476       0  173.67   69.40          0.121528         0.0   \n",
              "2     72  33.702285       0  177.47  105.70          0.143750         0.0   \n",
              "3     81  20.274075       0  171.74   61.10          0.664583         0.0   \n",
              "4     41  29.027749       1  175.75   90.00          0.004167         0.0   \n",
              "..   ...        ...     ...     ...     ...               ...         ...   \n",
              "895   61  22.840598       0  174.96   71.20          0.009722         0.0   \n",
              "896   74  28.843833       1  169.31   82.90          0.389583         0.0   \n",
              "897   68  22.744572       1  170.02   67.05          2.172917         0.0   \n",
              "898   55  25.356784       0  169.90   73.90          4.186111         0.0   \n",
              "899   62  16.720580       1  165.40   46.00          0.157639         0.0   \n",
              "\n",
              "     bun_apache  creatinine_apache  gcs_eyes_apache  ...  sodium_apache  \\\n",
              "0            50               3.32              1.0  ...            135   \n",
              "1            33               1.40              1.0  ...            133   \n",
              "2            17               1.71              1.0  ...            143   \n",
              "3            35               2.09              3.0  ...            136   \n",
              "4             3               0.41              1.0  ...            149   \n",
              "..          ...                ...              ...  ...            ...   \n",
              "895          22               0.90              4.0  ...            134   \n",
              "896          12               0.74              3.0  ...            144   \n",
              "897          36               1.87              3.0  ...            137   \n",
              "898          12               1.38              4.0  ...            139   \n",
              "899          11               2.80              4.0  ...            135   \n",
              "\n",
              "     temp_apache  ventilated_apache  wbc_apache  \\\n",
              "0           33.0                1.0        14.8   \n",
              "1           32.1                1.0        12.5   \n",
              "2           33.9                1.0        17.8   \n",
              "3           36.4                1.0         9.0   \n",
              "4           32.3                1.0        24.0   \n",
              "..           ...                ...         ...   \n",
              "895         36.5                0.0         5.2   \n",
              "896         36.4                1.0         9.9   \n",
              "897         35.8                1.0        29.7   \n",
              "898         35.9                1.0        15.3   \n",
              "899         36.2                1.0         5.8   \n",
              "\n",
              "     apache_4a_hospital_death_prob  apache_4a_icu_death_prob  aids  cirrhosis  \\\n",
              "0                             0.84                      0.71   0.0        0.0   \n",
              "1                             0.70                      0.54   0.0        0.0   \n",
              "2                             0.49                      0.29   0.0        0.0   \n",
              "3                             0.40                      0.31   0.0        0.0   \n",
              "4                             0.71                      0.64   0.0        0.0   \n",
              "..                             ...                       ...   ...        ...   \n",
              "895                           0.04                      0.03   0.0        0.0   \n",
              "896                           0.02                      0.00   0.0        0.0   \n",
              "897                           0.16                      0.11   0.0        0.0   \n",
              "898                           0.01                     -0.02   0.0        0.0   \n",
              "899                           0.08                      0.04   0.0        0.0   \n",
              "\n",
              "     diabetes_mellitus  leukemia  \n",
              "0                  0.0       0.0  \n",
              "1                  1.0       0.0  \n",
              "2                  0.0       0.0  \n",
              "3                  0.0       0.0  \n",
              "4                  0.0       0.0  \n",
              "..                 ...       ...  \n",
              "895                0.0       0.0  \n",
              "896                0.0       0.0  \n",
              "897                1.0       0.0  \n",
              "898                0.0       0.0  \n",
              "899                0.0       0.0  \n",
              "\n",
              "[900 rows x 29 columns]"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_testing_data = pd.read_csv('lab2_advanced_testing.csv')\n",
        "advanced_testing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQdTAvOaIh6"
      },
      "source": [
        "### Step 2: Split training data into training and validation set (Optional)\n",
        "> You can split the training data into training and validation set, this is up to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "iMan7jJ-aKX9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "training_data = advanced_training_data.iloc[:int(len(advanced_training_data)*0.8)]\n",
        "validation_data = advanced_training_data.iloc[int(len(advanced_training_data)*0.8):]\n",
        "print(len(training_data) + len(validation_data) == len(advanced_training_data))\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY8HvRY4af3u"
      },
      "source": [
        "### Step 3: Build a Random Forest\n",
        "\n",
        "Define the attributions of the random forest\n",
        "> * You **can** modify the values of these attributes in advanced part\n",
        "> * Each tree can have different attribute values\n",
        "> * Must use function *build_tree()* to build a random forest model\n",
        "> * Must print out the *selected_datas* and *selected_features*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "xK0iM7goa2pj"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "# Define the attributes\n",
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "\n",
        "# total number of trees in a random forest\n",
        "n_trees = 5\n",
        "\n",
        "# number of features to train a decision tree\n",
        "n_features = int(np.sqrt(training_data.shape[1]))\n",
        "\n",
        "# the ratio to select the number of instances\n",
        "sample_size = 0.2\n",
        "n_samples = int(training_data.shape[0] * sample_size)\n",
        "\n",
        "seed = 0\n",
        "\n",
        "x_validation = validation_data.drop(['hospital_death'], axis=1)\n",
        "y_validation = validation_data['hospital_death'].values.flatten()\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "i1I7wbpWa_N1"
      },
      "outputs": [],
      "source": [
        "def build_forest(data, n_trees, n_features, n_samples):\n",
        "  \"\"\"\n",
        "  This function will build a random forest.\n",
        "  args:\n",
        "  * data: all data that can be used to train a random forest\n",
        "  * n_trees: total number of tree\n",
        "  * n_features: number of features\n",
        "  * n_samples: number of instances\n",
        "  return:\n",
        "  * forest: a random forest with 'n_trees' of decision tree\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  data_len = data.shape[0]\n",
        "  feature_list = ['age', 'bmi', 'gender', 'height', 'weight', 'pre_icu_los_days', 'arf_apache', \\\n",
        "    'bun_apache', 'creatinine_apache', 'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache', \\\n",
        "      'gcs_verbal_apache', 'glucose_apache', 'heart_rate_apache', 'hematocrit_apache', \\\n",
        "        'intubated_apache', 'map_apache', 'resprate_apache', 'sodium_apache', 'temp_apache', \\\n",
        "          'ventilated_apache', 'wbc_apache', 'apache_4a_hospital_death_prob', \\\n",
        "            'apache_4a_icu_death_prob', 'aids', 'cirrhosis', 'diabetes_mellitus', 'leukemia']\n",
        "  forest = []\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  # Create 'n_trees' number of trees and store each into the 'forest' list\n",
        "  for i in range(n_trees):\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Select 'n_samples' number of samples and 'n_features' number of features\n",
        "    # (you can select randomly or use any other techniques)\n",
        "    random.seed(seed)\n",
        "    selected_datas = random.sample(range(data_len), n_samples)\n",
        "    selected_features = random.sample(feature_list, n_features)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # print(f\"selected_datas = {selected_datas}\")\n",
        "    # print(f\"selected_features = {selected_features}\")\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Store the rows in 'selected_datas' from 'data' into a new DataFrame\n",
        "    tree_data = pd.DataFrame()\n",
        "    tree_data = data.loc[selected_datas]\n",
        "\n",
        "    # Filter the DataFrame for specific 'selected_features' (columns)\n",
        "    tree_data = tree_data[selected_features + ['hospital_death']]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Then use the new data and 'build_tree' function to build a tree\n",
        "    tree = build_tree(tree_data, max_depth, min_samples_split, depth)\n",
        "    # print(tree)\n",
        "\n",
        "    # Save your tree\n",
        "    forest.append(tree)\n",
        "\n",
        "  return forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "WvV8U7C2bIqO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected_datas = [2037, 1315, 3942, 6153, 4685, 5737, 5769, 5716, 2513, 3310, 3302, 2337, 1867, 2694, 1332, 2785, 4204, 692, 3487, 2838, 4527, 1403, 4027, 187, 4378, 259, 2175, 515, 2585, 6775, 1771, 3993, 1079, 2625, 4761, 475, 5475, 4578, 6635, 4956, 3579, 323, 6194, 6040, 5154, 964, 4691, 450, 782, 1736, 4408, 970, 5069, 2594, 421, 4485, 357, 253, 1062, 5946, 2684, 4395, 4373, 322, 5578, 6552, 4552, 6577, 535, 5049, 5988, 1910, 5210, 6660, 982, 3815, 6452, 104, 988, 2558, 2833, 2882, 3102, 5408, 5103, 6010, 5172, 4348, 5419, 6127, 434, 739, 479, 4541, 1837, 3120, 6029, 1938, 5435, 2335, 690, 4502, 5177, 367, 4571, 658, 4328, 6573, 4483, 691, 536, 5527, 477, 4549, 3149, 5662, 4272, 2802, 6373, 5922, 4507, 6434, 6438, 4150, 955, 2078, 5561, 2212, 6489, 1508, 3986, 3367, 2627, 6262, 2355, 5829, 2587, 3595, 6161, 4786, 1585, 3593, 668, 4687, 5548, 4079, 3108, 1738, 4944, 6056, 4885, 3353, 5819, 5323, 146, 1626, 3990, 3147, 3690, 1439, 6704, 5691, 564, 5846, 1022, 409, 5961, 4579, 5687, 5245, 5112, 5658, 6487, 3788, 1502, 5509, 356, 1310, 5281, 4197, 6233, 1059, 4160, 1319, 908, 1174, 1919, 4369, 1717, 3985, 2400, 2393, 95, 2391, 2810, 6345, 3864, 5630, 3331, 4151, 1024, 5723, 1928, 1394, 758, 3003, 2987, 2404, 4906, 2108, 4994, 3550, 170, 6715, 3988, 6537, 4115, 2468, 363, 2280, 3196, 5071, 6014, 6746, 2974, 6637, 6419, 4999, 5190, 6354, 4084, 4753, 2861, 6291, 2554, 6657, 476, 309, 726, 159, 3684, 4586, 2090, 4927, 2601, 899, 2383, 760, 1147, 1786, 4421, 1527, 3862, 981, 5311, 3894, 4382, 645, 5824, 6172, 3519, 2776, 2868, 1521, 2406, 2661, 2777, 6377, 5238, 6390, 3377, 2892, 3602, 139, 1342, 2784, 5516, 4243, 3656, 3328, 4581, 2551, 491, 3923, 4131, 1380, 6504, 1318, 514, 1537, 4862, 1080, 6741, 3945, 4121, 811, 4472, 1255, 3297, 4512, 445, 2114, 1855, 1267, 5194, 5724, 2657, 3257, 6650, 3861, 3889, 3064, 4826, 5635, 172, 2098, 4531, 5236, 447, 2152, 5502, 3484, 2263, 3756, 1642, 3354, 3704, 3117, 2374, 5497, 2608, 3935, 5021, 2164, 5308, 1044, 2841, 2884, 3066, 2240, 3230, 6413, 4714, 1569, 5470, 3299, 4247, 4807, 6225, 6248, 3536, 2604, 5029, 3482, 5340, 416, 5068, 5278, 670, 413, 5814, 703, 6529, 1326, 6403, 1252, 3008, 6020, 113, 439, 1533, 6025, 680, 653, 3320, 6271, 1930, 2066, 2769, 2890, 2977, 67, 6499, 2285, 1697, 3388, 4103, 5139, 4888, 1194, 5468, 84, 5339, 995, 5168, 5627, 1927, 2938, 2220, 6113, 3106, 4726, 1604, 3515, 4275, 4261, 2313, 4823, 2112, 3079, 3221, 1534, 5334, 2767, 4416, 2577, 5970, 781, 4594, 6410, 3, 578, 1349, 3995, 5359, 5465, 5885, 3664, 6749, 4380, 1434, 2565, 1088, 3075, 4029, 4535, 2662, 4406, 3961, 5161, 902, 1181, 6404, 730, 481, 5692, 224, 4342, 5913, 3390, 625, 2067, 5105, 5361, 3577, 209, 3727, 2344, 1175, 3372, 5923, 4180, 3531, 64, 815, 979, 2733, 1104, 3409, 201, 854, 5298, 3447, 3943, 6173, 1637, 6365, 2283, 1951, 6145, 3156, 5616, 6282, 3012, 1209, 6485, 3820, 1166, 5070, 6360, 5968, 3370, 4347, 3790, 3274, 3647, 867, 3464, 4457, 1583, 1666, 3893, 558, 4651, 3239, 231, 59, 883, 62, 2754, 6100, 4394, 3758, 6466, 293, 5111, 4065, 2087, 5782, 835, 6358, 3235, 5495, 1986, 3476, 2357, 5664, 4000, 5971, 470, 1603, 3946, 1714, 6398, 2526, 4799, 4465, 6405, 3034, 2827, 2214, 486, 103, 3316, 2363, 4569, 5647, 3711, 3617, 1138, 709, 1675, 4314, 3113, 2237, 5644, 1254, 1780, 6150, 6484, 5541, 4630, 6290, 2216, 1593, 2522, 6106, 3439, 592, 1306, 1012, 1192, 3699, 733, 1918, 6258, 3855, 3159, 2274, 4702, 576, 6501, 6337, 5831, 1941, 2451, 5593, 640, 5594, 959, 1548, 405, 6455, 4673, 657, 1629, 4337, 4898, 4983, 3743, 3737, 3597, 2649, 4699, 4, 3492, 5589, 627, 4113, 180, 2420, 4031, 5076, 2527, 6052, 3606, 2322, 4244, 1346, 6584, 14, 4138, 1891, 2501, 6314, 6229, 2023, 5483, 5763, 6259, 6075, 1601, 121, 6057, 6479, 3870, 2917, 2317, 3209, 438, 2783, 4074, 5950, 1293, 3429, 44, 4895, 4679, 2466, 3779, 2366, 2102, 656, 1671, 6441, 3383, 4339, 2786, 1208, 1762, 5977, 4185, 784, 1940, 4234, 156, 4063, 48, 769, 208, 4744, 1033, 379, 3115, 602, 5571, 5378, 1429, 1591, 6469, 2610, 513, 4602, 1127, 503, 3167, 1874, 2176, 4593, 1058, 3916, 6796, 1278, 5854, 5227, 824, 5425, 2092, 5158, 2553, 6556, 3406, 2510, 3849, 122, 3140, 4838, 12, 2287, 6726, 6506, 2929, 2429, 4972, 1446, 2202, 6348, 6392, 3332, 4565, 859, 1041, 2976, 5251, 6344, 2341, 4920, 1568, 1031, 566, 4240, 210, 3402, 2883, 6432, 2049, 2300, 1038, 2086, 134, 1684, 2631, 5085, 2044, 3798, 611, 4098, 2626, 6453, 567, 2511, 6615, 5893, 3325, 2056, 417, 3660, 4676, 1757, 5228, 5122, 4297, 1015, 5634, 5513, 5043, 2736, 4548, 1849, 361, 6389, 2774, 802, 3199, 662, 199, 3504, 2634, 3802, 5097, 731, 3850, 5987, 2445, 6239, 6164, 3131, 202, 3078, 2416, 5181, 5728, 6035, 4582, 6595, 894, 4250, 3030, 3016, 4567, 1884, 3636, 3179, 6234, 2647, 2001, 3723, 2624, 4177, 792, 574, 2530, 938, 6235, 3266, 4096, 654, 2232, 6149, 2567, 746, 3747, 3242, 5106, 6201, 2687, 252, 1448, 6160, 5528, 2797, 406, 3905, 1074, 5321, 2302, 2336, 1144, 5001, 1143, 5582, 3590, 5898, 4069, 3573, 5058, 1146, 58, 4890, 5535, 5438, 3084, 1510, 2297, 2507, 5847, 6047, 6648, 1093, 1954, 463, 3710, 4398, 3871, 2893, 3682, 4412, 5013, 1028, 1865, 6782, 1142, 2972, 2284, 796, 1399, 1948, 6179, 901, 1249, 4989, 4257, 6516, 5056, 1800, 5949, 6093, 4058, 2015, 6444, 1308, 2354, 3683, 6117, 3857, 6465, 1621, 2469, 1576, 1124, 3254, 6781, 196, 2660, 2831, 6296, 6240, 2211, 1274, 6716, 6533, 137, 432, 3410, 4009, 3132, 2378, 2764, 5100, 6267, 2298, 1707, 4655, 661, 4183, 2065, 1933, 233, 516, 4046, 4509, 3220, 1183, 1313, 4677, 1868, 2128, 436, 4068, 6698, 1391, 847, 4982, 4757, 4266, 1834, 1792, 2153, 6623, 332, 2310, 3841, 5469, 5428, 831, 5490, 2039, 6714, 2073, 5604, 192, 3808, 1895, 2047, 2441, 5496, 1411, 3308, 3448, 5145, 6051, 2863, 3070, 4620, 6450, 2999, 4153, 5276, 155, 697, 3258, 1906, 4540, 5180, 5624, 5330, 4479, 4675, 5702, 3229, 5342, 904, 1234, 1824, 5272, 4590, 2005, 3245, 3566, 1770, 3000, 6114, 4610, 3635, 3809, 935, 511, 2103, 2316, 4926, 1103, 4718, 2057, 1660, 2210, 1733, 5137, 3371, 4111, 4215, 1619, 4867, 2118, 1320, 1860, 2409, 4245, 6418, 5195, 688, 6540, 4708, 1517, 779, 1763, 343, 1887, 6043, 595, 5747, 3931, 6462, 335, 6599, 3249, 1140, 4224, 6395, 2159, 1061, 740, 1176, 6072, 154, 1612, 3547, 1963, 5748, 1567, 4252, 2801, 3963, 4263, 3584, 1035, 5540, 2712, 1987, 5335, 50, 905, 2913, 6386, 6249, 3915, 2575, 1097, 5222, 41, 1491, 3071, 6424, 269, 3866, 2249, 4641, 1177, 2717, 111, 5934, 5952, 165, 6059, 4253, 3540, 5072, 4637, 2642, 2731, 4108, 1048, 6546, 5128, 1011, 1199, 4427, 3741, 5921, 6063, 5823, 3203, 4584, 6591, 2398, 3558, 5804, 821, 6710, 6088, 1379, 557, 280, 6528, 3875, 1661, 3773, 5749, 5926, 555, 4634, 1598, 3562, 6388, 3801, 5733, 4359, 5706, 152, 2821, 4517, 2165, 2479, 4568, 3054, 589, 4901, 719, 1300, 3661, 1355, 2169, 6762, 7, 3592, 5488, 1560, 6318, 3163, 6308, 4542, 1777, 1451, 4277, 4248, 3782, 6200, 386, 575, 3598, 4897, 4195, 6380, 227, 892, 2932, 5542, 6217, 1123, 391, 1667, 2613, 110, 1741, 852, 6426, 5358, 375, 5297, 2721, 2402, 3934, 4935, 3255, 6313, 5584, 2817, 5915, 2, 2491, 57, 3164, 1547, 418, 5919, 6476, 6016, 3730, 5102, 2289, 1825, 4619, 4415, 28, 2899, 1219, 320, 1695, 3042, 2183, 4669, 1706, 912, 6564, 4164, 4834, 2258, 3888, 4101, 3775, 5250, 351, 2075, 1961, 973, 1160, 4748, 5592, 870, 2855, 168, 96, 1650, 420, 1129, 5879, 2848, 4598, 3694, 3578, 5866, 545, 1952, 319, 3702, 1406, 2980, 2557, 2290, 5114, 3644, 411, 2370, 161, 1196, 2026, 4877, 2941, 1407, 6495, 4773, 3206, 2727, 4042, 2865, 6755, 3336, 4056, 4004, 6442, 6383, 1743, 3019, 629, 4917, 3405, 1445, 1718, 2744, 1477, 2190, 5304, 2053, 3669, 5570, 6651, 3362, 396, 1159, 2343, 3948, 2131, 1782, 4501, 594, 3373, 5700, 4235, 2815, 1203, 6443, 4025, 2751, 3295, 4057, 2035, 3637, 6238, 3803, 2597, 1265, 2580, 1064, 5873, 1424, 466, 4559, 5577, 6305, 1494, 1524, 5863, 814, 6230, 994, 2982, 2372, 3006, 1556, 3885, 5998, 3063, 2178, 264, 3563, 5767, 1240, 4560, 6325, 6689, 5639, 3215, 1250, 3687, 5135, 4352, 2080, 6320, 1861, 5036, 4784]\n",
            "selected_features = ['pre_icu_los_days', 'apache_4a_hospital_death_prob', 'gcs_verbal_apache', 'leukemia', 'diabetes_mellitus']\n",
            "{'apache_4a_hospital_death_prob <= 0.235': [0, 1]}\n",
            "selected_datas = [3350, 3617, 3232, 2178, 4188, 2586, 6788, 4087, 1381, 6035, 4751, 3449, 4635, 6020, 4517, 4512, 25, 1739, 6188, 3443, 3556, 2702, 6023, 182, 662, 3406, 1972, 3394, 1885, 4123, 2453, 5118, 544, 5149, 1593, 5000, 2739, 5008, 3054, 247, 4139, 545, 274, 6640, 5846, 2060, 1531, 8, 5292, 3933, 6152, 2457, 5966, 4432, 3969, 2937, 2893, 5134, 2696, 1087, 5845, 1410, 1902, 2853, 6282, 758, 2371, 1276, 786, 936, 5671, 4375, 189, 2516, 744, 1222, 525, 4238, 1567, 6373, 2802, 4391, 1021, 1232, 5665, 5411, 5344, 3628, 1908, 2420, 4718, 2814, 3005, 3705, 6339, 6214, 1982, 4728, 3981, 2011, 5390, 1184, 2423, 3025, 5238, 3780, 2639, 2229, 5126, 4893, 6354, 5593, 4235, 6475, 6136, 4010, 5688, 647, 4777, 3896, 4445, 2209, 4482, 6326, 4884, 5116, 510, 5849, 1605, 1514, 6350, 658, 4170, 4879, 2313, 5479, 3212, 2279, 3677, 4212, 3947, 2427, 4590, 3418, 3072, 6101, 2576, 3366, 1946, 54, 3274, 3926, 5534, 1252, 4941, 1433, 2091, 3849, 6029, 6049, 3046, 1832, 3649, 827, 4223, 5264, 5389, 2675, 629, 4969, 1300, 4786, 1846, 5308, 6723, 2611, 5720, 5039, 4263, 4268, 4686, 1918, 4889, 4379, 3260, 1523, 2905, 5293, 4950, 6025, 1604, 2045, 4260, 2932, 5434, 1485, 4651, 1298, 2560, 5804, 2366, 1074, 3908, 4666, 4778, 5150, 6598, 515, 4242, 2064, 1827, 2792, 239, 1680, 872, 6388, 2575, 5355, 4732, 4908, 4615, 990, 3510, 1066, 3029, 3770, 2800, 6165, 5185, 3451, 3598, 3157, 148, 1958, 4547, 1490, 6691, 5664, 5970, 5624, 2920, 1425, 6612, 2041, 876, 4994, 4392, 4864, 3446, 3892, 5214, 5168, 3410, 1482, 2302, 5790, 2014, 4503, 5298, 533, 6312, 5679, 2826, 5677, 1364, 4293, 1533, 2150, 6601, 6186, 1024, 1750, 2892, 1314, 5447, 4839, 2980, 6529, 3959, 680, 6016, 6711, 2728, 3477, 446, 5779, 160, 1280, 3552, 127, 2463, 5596, 6005, 6199, 6707, 3349, 3531, 4773, 6277, 358, 3900, 6316, 2681, 5531, 6147, 5805, 123, 2252, 4041, 592, 1071, 46, 1805, 2010, 2193, 722, 4205, 5640, 4932, 1691, 2006, 3426, 1863, 232, 4257, 6660, 6012, 1861, 3600, 5694, 6616, 3055, 1258, 4430, 298, 2386, 4378, 488, 4090, 227, 36, 2092, 5537, 6372, 1904, 6371, 1655, 1455, 3302, 2648, 3588, 6464, 4568, 4425, 6513, 3253, 4442, 4049, 4529, 2723, 1371, 5091, 1339, 3049, 1973, 6021, 4757, 3970, 5632, 6572, 1966, 341, 3776, 6151, 4563, 815, 1971, 1792, 2862, 3066, 6676, 6275, 5362, 4934, 4406, 6285, 6534, 1118, 4513, 2745, 5327, 2172, 3263, 3415, 5450, 2542, 2774, 878, 1065, 2430, 3935, 4478, 6167, 2708, 6484, 2287, 4347, 3543, 3305, 3958, 5038, 3594, 2210, 3777, 1316, 3986, 2617, 5229, 2050, 2140, 1062, 4656, 1818, 1882, 1257, 4191, 3026, 2904, 2959, 505, 6353, 6649, 5709, 5188, 6091, 996, 2044, 867, 1633, 2297, 698, 380, 5057, 946, 5837, 3515, 3173, 429, 2277, 3292, 2887, 6508, 2330, 3155, 5332, 728, 703, 3126, 6782, 273, 960, 2561, 5574, 6583, 2066, 3504, 1309, 284, 5259, 2089, 1108, 191, 3503, 5791, 783, 2786, 6620, 1141, 2132, 5397, 5037, 3946, 4959, 2933, 762, 6755, 4303, 6417, 6317, 4695, 5773, 1368, 5123, 230, 2605, 5394, 4046, 5535, 5119, 2593, 1603, 3041, 1251, 3328, 6790, 814, 1113, 1840, 6658, 4679, 34, 1218, 2859, 4904, 1185, 2864, 3382, 5469, 17, 916, 254, 5980, 6629, 6527, 3180, 3766, 2793, 1631, 3875, 5547, 5066, 1920, 3773, 5471, 499, 5941, 578, 5223, 1950, 217, 3940, 2179, 6543, 2236, 1423, 6485, 5950, 258, 275, 55, 5096, 6638, 4202, 2747, 587, 5927, 3416, 6232, 2982, 4885, 4987, 3178, 6442, 4016, 2991, 2153, 2418, 5673, 329, 1461, 1265, 107, 1078, 324, 4795, 904, 4386, 4064, 3599, 6611, 726, 1900, 5758, 6662, 3883, 1295, 5555, 4664, 2821, 1794, 86, 52, 5102, 3829, 5579, 4983, 2536, 31, 4970, 873, 1785, 1939, 3271, 3429, 2099, 3079, 2216, 3229, 5725, 250, 4949, 2339, 5049, 2867, 5085, 3458, 3353, 5776, 3533, 3815, 1027, 1244, 1755, 62, 2621, 2442, 6301, 1133, 4444, 3021, 301, 5239, 3866, 1494, 214, 4533, 406, 3603, 2367, 910, 2497, 4164, 6097, 2061, 1776, 902, 310, 6668, 5451, 1183, 6680, 5527, 3577, 565, 2275, 4916, 3264, 6066, 1996, 5501, 1274, 2319, 1841, 6063, 1096, 5333, 495, 6528, 2691, 5, 5523, 3202, 334, 1802, 3492, 818, 6557, 5023, 6542, 2854, 2985, 3185, 3775, 6094, 3456, 623, 4791, 5667, 2647, 665, 6118, 1031, 3848, 4158, 5305, 3240, 2177, 2664, 6754, 2058, 4277, 6521, 2569, 6739, 248, 5543, 3146, 915, 5726, 2324, 2358, 4195, 3428, 3642, 222, 4300, 4583, 1260, 5060, 3826, 2803, 4068, 705, 5477, 1469, 2095, 4532, 6268, 4787, 2451, 715, 452, 6720, 4278, 1109, 4849, 5583, 5016, 306, 2559, 4601, 1486, 2071, 2961, 737, 20, 4685, 987, 4620, 4501, 3957, 3068, 3508, 2148, 3363, 1402, 145, 5495, 6622, 5820, 3282, 297, 2943, 1704, 1182, 1345, 1346, 3648, 2432, 2322, 2509, 945, 733, 2908, 546, 3907, 63, 4921, 2742, 294, 300, 2657, 4267, 6749, 6161, 405, 5346, 4349, 922, 135, 2672, 3629, 6746, 4535, 1212, 3345, 5890, 492, 970, 403, 6701, 3640, 5623, 2126, 5857, 928, 3724, 104, 756, 5646, 4074, 4321, 362, 3500, 6448, 6522, 5483, 6694, 6040, 2213, 255, 5137, 2787, 6688, 6780, 2872, 2205, 4333, 2123, 3225, 1611, 5809, 5948, 3555, 5408, 5754, 3217, 974, 1115, 4228, 4776, 2818, 4470, 4304, 350, 169, 1242, 5241, 205, 4843, 1327, 3283, 2012, 2698, 4553, 1743, 394, 4318, 5939, 2190, 1104, 3171, 3647, 4106, 1666, 3122, 4104, 5635, 1293, 4118, 2667, 5076, 6415, 3272, 2600, 3354, 6709, 5958, 745, 3728, 4721, 3267, 4005, 6068, 3740, 952, 1377, 6218, 5813, 3326, 3715, 3778, 2634, 3489, 3056, 96, 6045, 3666, 1789, 1862, 3148, 5721, 5335, 2941, 3275, 340, 6724, 977, 1239, 4291, 6573, 927, 2399, 1773, 6013, 3535, 3348, 4370, 5337, 6751, 353, 6123, 3154, 1698, 3810, 5830, 513, 645, 1386, 2768, 6554, 4793, 5600, 2174, 4420, 4641, 4682, 3108, 339, 937, 3751, 6124, 1769, 4494, 6722, 6608, 5267, 3099, 3882, 4646, 2331, 2889, 994, 1894, 741, 1483, 3560, 2619, 4938, 12, 1465, 2008, 4219, 4448, 864, 3790, 3476, 15, 282, 2638, 6797, 6689, 2345, 3832, 2875, 2841, 5421, 5470, 954, 1886, 1893, 1584, 4570, 4543, 1726, 2003, 3517, 3192, 2175, 2643, 5822, 972, 2726, 6134, 5960, 4382, 4794, 4537, 6099, 2769, 1, 3083, 5870, 342, 1424, 4762, 1634, 3009, 3733, 6150, 4997, 1488, 167, 1839, 5213, 1342, 50, 4140, 4958, 3697, 2518, 5559, 2567, 3245, 804, 1215, 366, 3485, 1367, 2272, 1833, 197, 6296, 2437, 4488, 4817, 5839, 5206, 5963, 6493, 2458, 5006, 833, 632, 4224, 2656, 3707, 4264, 5873, 4199, 6523, 5248, 4599, 2796, 4803, 3031, 2419, 4712, 2558, 3183, 1872, 642, 3430, 2360, 6242, 2401, 2689, 4991, 1111, 3963, 5746, 483, 6418, 2824, 905, 2852, 3706, 1600, 562, 390, 6074, 6309, 3073, 6267, 5577, 6421, 3537, 1468, 3076, 3365, 4438, 2962, 3308, 676, 2910, 1029, 6046, 5474, 551, 1385, 1332, 4451, 3890, 3747, 1922, 6252, 3276, 2489, 2640, 4780, 5018, 826, 4495, 2038, 3403, 6120, 3865, 4437, 1003, 5175, 1434, 1037, 1767, 3708, 2988, 1933, 6198, 2789, 4015, 5674, 5814, 2623, 2155, 3045, 6671, 3823, 1057, 1810, 2163, 4848, 2414, 708, 5089, 5566, 4622, 3411, 1891, 2839, 6253, 173, 2604, 175, 6495, 2115, 3237, 1820, 2545, 170, 640, 5127, 2957, 663, 2843, 99, 2200, 59, 1414, 2026, 570, 2987, 4314, 2830, 6233, 2502, 377, 6293, 2127, 4900, 6216, 5844, 3693, 588, 2662, 1675, 5502, 3562, 1890, 465, 1550, 1117, 4743, 5269, 444, 6627, 3398, 4573, 3312, 5440, 5012, 5732, 1103, 1612, 511, 3124, 4337, 1060, 796, 3343, 5769, 6056, 6450, 476, 6520, 1440, 1002, 276, 3223, 1813, 1059, 5541, 688, 2485, 2886, 5192, 4380, 1175, 5456, 3207, 883, 3327, 5606, 2997, 1717, 3622, 5216, 6376, 57, 5406, 5522, 5136, 4716, 4500, 3386, 5833, 1513, 992, 1517, 5896, 6605, 1165, 396, 2725, 5045, 4368, 1834, 3912, 5181, 4003, 3261, 1380, 4135, 3218, 5240, 1724, 2658, 4028, 1599, 4819, 6408, 6603, 347, 896, 6716, 6055, 3806, 567, 1592, 4986, 45, 6635, 4361, 5320, 3340, 5376, 2481, 2871, 5478, 6014, 6168, 2220, 2811, 5854, 1537, 372, 1571, 5351, 1957, 6506, 2260, 2731, 3501, 4418, 3497, 401, 5645, 3453, 5768, 6321, 2219, 1682, 1094, 6503, 3994, 2035, 2644, 1174, 685, 4401, 1683, 3717, 5350, 822, 5599, 2595, 5044, 1575, 3581, 2370, 1443, 1179, 110, 2950, 3779, 997, 5750, 1006, 3486, 3250, 5217, 2968, 1822, 6672, 79, 709, 4509, 3862, 2758, 917, 2286, 5856, 6143, 5561, 6194, 2635, 1919, 3335, 4066, 3833, 1751, 1855]\n",
            "selected_features = ['gcs_motor_apache', 'apache_4a_hospital_death_prob', 'gcs_eyes_apache', 'gcs_verbal_apache', 'apache_4a_icu_death_prob']\n",
            "{'apache_4a_icu_death_prob <= 0.07500000000000001': [0, 1]}\n",
            "selected_datas = [818, 1876, 4517, 3042, 2870, 6588, 2278, 933, 6319, 5934, 2834, 2289, 1658, 2370, 6153, 6261, 740, 1821, 1677, 827, 5814, 5488, 5655, 4986, 5012, 4806, 4727, 3062, 6328, 2132, 6008, 6133, 2903, 708, 1648, 4798, 6037, 3130, 1718, 4695, 327, 121, 5107, 1356, 1484, 5548, 6060, 1766, 3267, 3074, 5473, 4796, 3122, 4499, 6537, 2994, 586, 4314, 6524, 2201, 2264, 3379, 4587, 5595, 2479, 962, 4920, 4260, 3070, 5130, 3096, 3233, 4, 3189, 2456, 3149, 2011, 4825, 2804, 5156, 4683, 3930, 5003, 1941, 2679, 432, 3505, 1650, 1052, 969, 3639, 4037, 4173, 1574, 6323, 2257, 3220, 5667, 1247, 6058, 2773, 2945, 463, 4199, 5889, 1472, 938, 1962, 5088, 2612, 4803, 3695, 4631, 2355, 2223, 1716, 2102, 1694, 3849, 5923, 1815, 2508, 6113, 5901, 6790, 4467, 4230, 2540, 6771, 5713, 1513, 3952, 3104, 2450, 6515, 4241, 1146, 1760, 2323, 3, 333, 1045, 2072, 6152, 3307, 3281, 3947, 2638, 4994, 2403, 3545, 1068, 6552, 4391, 1669, 117, 4561, 2565, 3328, 5852, 3894, 934, 6719, 1128, 4660, 5979, 6013, 4603, 622, 6734, 5965, 1583, 287, 2322, 3446, 318, 1900, 3901, 364, 2326, 97, 3515, 2251, 6126, 38, 2562, 47, 6128, 3853, 6710, 2210, 6094, 858, 1985, 2047, 1663, 3236, 1009, 6747, 508, 5167, 2851, 2086, 5758, 4595, 4535, 3309, 2050, 2196, 1471, 3847, 3861, 3721, 4878, 900, 4081, 157, 5330, 3222, 5024, 3008, 1196, 5506, 2368, 3573, 838, 1613, 6142, 784, 6315, 4179, 2493, 6016, 5478, 5421, 3280, 5035, 6671, 6244, 6159, 4833, 6474, 5690, 4293, 5074, 2280, 191, 5201, 6695, 3049, 765, 2439, 6462, 2636, 6348, 2018, 2224, 3002, 3495, 6409, 5651, 3564, 1362, 876, 6164, 6717, 5297, 1189, 1919, 1886, 2381, 1582, 562, 401, 1475, 2778, 416, 266, 102, 6052, 5138, 1085, 3832, 6538, 2677, 3759, 4253, 470, 246, 1788, 2577, 4817, 2629, 4859, 43, 2538, 4728, 6225, 4273, 2650, 1906, 2973, 4207, 2549, 138, 6420, 1030, 3320, 3078, 5016, 4656, 4801, 5527, 5456, 4201, 3414, 6377, 4762, 2510, 6482, 2316, 5489, 5756, 234, 2850, 349, 3783, 813, 3845, 2590, 2300, 4407, 2375, 1357, 774, 772, 1935, 6049, 3422, 1592, 5691, 3334, 2259, 3039, 5851, 3644, 1288, 182, 2996, 684, 398, 6477, 2491, 3799, 25, 6066, 2037, 6069, 6390, 2663, 2000, 1681, 6422, 981, 2595, 4553, 1476, 3933, 2656, 3601, 493, 460, 1922, 3398, 6766, 961, 4960, 4034, 1, 907, 3491, 2925, 874, 5956, 1504, 873, 2371, 309, 1598, 1843, 6504, 2682, 6483, 4918, 1514, 5936, 4781, 383, 3071, 5328, 5714, 5190, 3442, 4221, 1071, 2512, 3148, 1032, 3606, 682, 3266, 851, 3107, 2221, 4945, 452, 5344, 742, 1409, 4529, 4242, 1105, 1528, 3101, 636, 6428, 6252, 4137, 5196, 4116, 6568, 2349, 1683, 2518, 6609, 5305, 3909, 3389, 4809, 6038, 5622, 5520, 4380, 1686, 5366, 5962, 6579, 6612, 1044, 2662, 205, 1682, 6192, 2232, 1180, 2021, 3294, 3029, 3141, 6121, 6191, 4644, 862, 3529, 101, 5783, 5517, 577, 6558, 6125, 4679, 4077, 4887, 739, 298, 6406, 2441, 2771, 5682, 948, 6640, 2573, 6074, 1754, 2567, 6240, 3465, 5369, 4417, 3915, 5179, 64, 6047, 5120, 1099, 3731, 556, 2692, 2414, 3217, 1581, 3364, 6532, 1286, 3587, 490, 3834, 144, 1969, 553, 3396, 1270, 2347, 5557, 6564, 5246, 16, 1706, 2830, 1277, 1710, 2108, 2891, 3100, 3295, 2485, 5796, 3862, 2743, 2321, 5099, 6135, 2253, 4148, 570, 4220, 4618, 673, 554, 3512, 3816, 996, 5092, 3158, 5375, 4098, 749, 4080, 6503, 3925, 4070, 960, 649, 671, 4703, 3554, 3976, 368, 2924, 5946, 5510, 5117, 451, 1328, 886, 1081, 664, 4671, 3371, 3432, 5108, 5862, 4106, 6762, 6017, 6089, 839, 4437, 4619, 4814, 4883, 1402, 2877, 5533, 3283, 5325, 4072, 4105, 1317, 624, 2474, 1543, 1532, 1997, 728, 1065, 3063, 5886, 5828, 95, 6224, 672, 1868, 2578, 6023, 6697, 2676, 831, 1090, 6521, 3324, 1079, 1916, 2109, 4107, 5039, 4846, 5938, 3391, 2420, 4597, 392, 1418, 6262, 1721, 4998, 6622, 5689, 4900, 4875, 716, 183, 1490, 4311, 3403, 3680, 958, 1395, 3533, 2673, 3355, 2190, 6195, 4580, 5293, 5544, 5028, 5813, 1188, 550, 203, 5687, 3155, 4705, 6627, 1580, 2199, 3032, 215, 173, 6592, 2427, 803, 2852, 4584, 3771, 5856, 4545, 5149, 5729, 3109, 5080, 676, 5102, 5304, 1794, 6272, 6036, 5483, 4031, 3069, 3353, 5043, 1800, 903, 1817, 3953, 3289, 3157, 6206, 5897, 815, 4572, 2181, 5757, 5089, 4240, 3859, 6087, 963, 2389, 2708, 2240, 6597, 6774, 970, 1461, 6073, 5523, 693, 3474, 662, 2822, 808, 4402, 2483, 2760, 604, 3584, 2287, 4717, 6025, 6778, 4361, 1725, 1875, 4637, 3252, 536, 5696, 3272, 6681, 3453, 3152, 976, 5636, 3857, 6607, 6274, 4310, 5104, 4999, 3617, 945, 163, 4485, 3424, 2522, 3527, 1826, 4513, 5742, 5806, 1006, 2896, 5197, 3261, 4673, 2528, 1925, 5228, 223, 2069, 4237, 3580, 125, 4256, 5426, 2020, 3447, 3866, 2911, 4192, 1822, 3369, 3808, 3557, 3184, 3658, 2121, 3727, 5062, 5611, 2035, 5386, 2868, 1235, 2974, 3650, 4022, 6389, 2801, 2862, 6626, 6139, 197, 738, 4370, 1296, 2848, 1488, 805, 3246, 2433, 4872, 769, 5712, 3665, 5084, 5760, 4184, 4460, 4375, 6522, 1014, 4502, 3748, 4544, 2207, 6250, 1793, 1744, 1993, 638, 211, 3377, 4399, 4217, 5224, 6005, 6085, 3486, 3870, 5274, 4294, 909, 920, 2882, 1257, 5177, 6735, 4896, 2154, 4023, 4978, 3480, 6103, 4718, 3542, 3343, 4931, 5556, 2032, 6625, 4964, 6263, 6072, 2151, 5780, 5652, 1556, 5781, 1135, 5031, 603, 3479, 208, 6554, 5895, 4008, 100, 6458, 4957, 4853, 6101, 5693, 2511, 1059, 6595, 4856, 4614, 4474, 2757, 1192, 2748, 4854, 776, 4319, 2816, 5614, 4277, 2295, 6637, 4263, 741, 6143, 691, 272, 464, 4577, 653, 6338, 517, 3785, 5739, 5236, 1606, 2723, 4316, 2054, 1078, 6491, 4024, 1291, 1816, 6346, 4821, 1140, 3498, 1153, 1974, 104, 268, 515, 6527, 3987, 786, 5536, 1109, 131, 1738, 5737, 3131, 1502, 1352, 2234, 613, 6688, 357, 2292, 1747, 3113, 579, 4749, 645, 3957, 475, 6215, 927, 6656, 301, 1101, 3373, 5245, 2761, 6534, 6667, 1624, 1662, 4861, 3950, 61, 3075, 1143, 2646, 5345, 650, 2242, 2842, 6761, 262, 2596, 6185, 6270, 3256, 3880, 3404, 4692, 3200, 4498, 2386, 5570, 1083, 48, 4112, 4477, 6650, 2058, 6394, 2704, 3089, 4882, 292, 2029, 4633, 5207, 1157, 6507, 3224, 4298, 4530, 41, 6614, 1226, 1887, 4445, 4787, 5356, 3898, 2849, 3732, 2285, 5119, 6789, 6566, 5370, 3944, 3921, 2598, 3291, 6327, 4979, 3232, 4737, 885, 4789, 2412, 4988, 2003, 3900, 4378, 4782, 4548, 4250, 2481, 4200, 4616, 2992, 995, 3630, 2353, 5125, 3752, 884, 6368, 4337, 1988, 2142, 1203, 4390, 1741, 6453, 4393, 2388, 1340, 6577, 3339, 1025, 5820, 4649, 2397, 2910, 5516, 3439, 4540, 3001, 2681, 1873, 3264, 4965, 4697, 2506, 677, 6468, 4209, 4841, 4405, 3055, 5434, 5253, 253, 1963, 5988, 1761, 1405, 2728, 5679, 6413, 394, 4053, 5160, 6247, 3329, 4458, 2306, 6539, 5447, 3991, 31, 6603, 2813, 3736, 1785, 4794, 3219, 1329, 1614, 2344, 4708, 4492, 4588, 2856, 430, 6647, 6201, 465, 861, 6796, 4444, 3418, 1228, 3133, 987, 2914, 4805, 1559, 5620, 3621, 2024, 4167, 1773, 3992, 1233, 5219, 3594, 931, 5777, 2066, 1585, 4381, 5752, 6745, 4659, 639, 1160, 1020, 3706, 510, 1260, 1934, 4890, 1161, 400, 321, 1122, 6605, 105, 1459, 4117, 6249, 3061, 5110, 6204, 2103, 1542, 4961, 4843, 5314, 6580, 444, 171, 4851, 6728, 5678, 1546, 5242, 419, 4987, 5870, 4709, 2374, 5439, 4462, 3208, 1530, 2025, 4915, 6267, 5754, 2968, 5022, 5880, 2471, 6197, 777, 6480, 628, 1879, 6242, 2031, 5292, 2805, 109, 5975, 5579, 3690, 123, 3195, 5266, 4793, 710, 3034, 1838, 3385, 5578, 605, 6635, 6219, 2002, 1844, 6379, 6718, 4369, 878, 5786, 2690, 1778, 3124, 4838, 2062, 646, 1016, 5591, 2774, 6148, 4909, 1067, 6654, 405, 3852, 3831, 4693, 3225, 1544, 535, 1635, 3794, 5525, 4712, 560, 3669, 1133, 5512, 1077, 2470, 781, 730, 1835, 4327, 6106, 85, 2472, 5218, 6282, 1767, 4763, 4312, 1529, 4432, 1525, 6571, 1932, 2675, 2746, 4366, 193, 1891, 3812, 2446, 80, 5389, 3316, 3423, 569, 1425, 4398, 656, 2553, 2628, 6584, 2726, 106, 6014, 951, 4395, 4374, 5186, 4161, 3939, 4510, 2274, 4463, 6294, 3689, 4823, 3310, 5577, 4512, 1299, 1561, 348, 5162, 4067, 3555, 6432, 3938, 2735, 5774, 6115, 3357, 3955, 1316, 3346, 922, 283, 3507, 584, 2772, 3139, 3647, 2602, 6059, 5821, 1427, 5044, 6054, 1784, 3260, 6290, 3153, 3162, 5347, 448, 3559, 1557, 5139, 4440, 1091, 1734, 2067, 1819, 5915, 5626, 3715, 5730, 3566, 5604, 375, 516, 2165]\n",
            "selected_features = ['apache_4a_hospital_death_prob', 'gcs_verbal_apache', 'leukemia', 'gcs_motor_apache', 'gender']\n",
            "{'apache_4a_hospital_death_prob <= 0.245': [0, 1]}\n",
            "selected_datas = [3852, 5626, 1059, 4345, 5852, 2405, 2807, 2186, 6280, 3370, 5391, 4821, 4395, 2621, 6784, 2133, 4313, 2079, 3725, 5776, 4091, 3984, 5427, 2910, 3856, 580, 6603, 2412, 1623, 5403, 3798, 2360, 2110, 5353, 4166, 4920, 5185, 682, 6291, 1126, 3703, 2029, 4308, 2053, 2401, 5313, 1042, 2528, 3114, 5692, 2871, 652, 3079, 3353, 3927, 4587, 5566, 1740, 3586, 6352, 1658, 1867, 6469, 3689, 1462, 5441, 5345, 983, 6605, 2518, 3290, 1518, 1350, 6717, 4436, 5558, 3118, 6448, 435, 1712, 2124, 4145, 2969, 781, 3002, 858, 1409, 6112, 3494, 4058, 5205, 3352, 118, 5375, 5246, 4365, 2532, 1994, 5350, 4115, 2637, 106, 419, 4206, 1822, 4600, 5219, 6353, 2178, 2374, 5820, 4448, 6528, 6081, 3294, 1779, 1452, 3841, 960, 6176, 1421, 1341, 2754, 6554, 1947, 4325, 5463, 3922, 6300, 181, 5139, 2997, 2669, 52, 681, 3935, 4593, 265, 948, 5661, 1085, 5389, 3675, 1654, 905, 6716, 1176, 1163, 2700, 2742, 5146, 6400, 6052, 4489, 4577, 5840, 1884, 1374, 2418, 1598, 5704, 5679, 4599, 863, 6250, 1700, 123, 2208, 6184, 4856, 1454, 1104, 6131, 2156, 5867, 5656, 6142, 6252, 2966, 785, 1400, 747, 6797, 3939, 5725, 3930, 2268, 6125, 2052, 3665, 1208, 5473, 5203, 2598, 947, 3808, 5622, 2495, 2423, 1453, 4937, 5060, 949, 1693, 4475, 6499, 5400, 4391, 6601, 3095, 1331, 6591, 3713, 1845, 4765, 2366, 970, 1498, 4692, 4195, 3217, 3695, 3150, 675, 4251, 4767, 2607, 360, 4199, 5306, 1439, 5423, 5436, 6429, 2913, 6777, 2897, 988, 4559, 99, 1995, 3101, 3475, 3086, 840, 3096, 565, 2015, 1564, 6553, 2276, 1978, 285, 2437, 5574, 5722, 1563, 415, 2358, 5149, 4396, 1762, 249, 2568, 1677, 1542, 4790, 6187, 1933, 376, 154, 605, 4465, 3732, 3812, 871, 5648, 1999, 1405, 5275, 1212, 4481, 363, 1193, 276, 638, 1703, 881, 1071, 499, 1076, 3787, 3444, 6526, 4015, 1099, 3745, 4725, 6772, 22, 5339, 3931, 5028, 4646, 532, 4685, 5703, 5501, 4737, 1064, 1678, 2505, 5095, 5471, 4650, 1588, 4363, 2113, 3333, 2930, 4121, 5912, 2152, 6295, 4230, 6180, 2083, 1318, 6505, 6027, 1108, 5513, 2524, 2664, 5053, 4294, 1800, 6133, 898, 2468, 6582, 1385, 1147, 3905, 6182, 392, 6072, 3915, 4383, 5571, 2185, 2345, 1862, 4714, 5785, 2601, 2931, 6408, 2221, 922, 470, 5267, 5627, 2373, 1358, 4612, 5285, 3243, 6425, 1628, 6639, 2507, 4854, 6418, 3403, 5973, 591, 2972, 1140, 5472, 752, 6440, 518, 5004, 5296, 4209, 5651, 6048, 2101, 6594, 3148, 5516, 4668, 344, 620, 4778, 5763, 5995, 5291, 5981, 5701, 4186, 1892, 5567, 2822, 6299, 2616, 1169, 3680, 6026, 1446, 1618, 643, 4965, 1190, 238, 5947, 1707, 5684, 2735, 2485, 772, 166, 5858, 4338, 2119, 6691, 3366, 273, 6028, 382, 4613, 1311, 825, 953, 990, 186, 833, 452, 5655, 4409, 6050, 1457, 357, 2241, 2733, 4181, 590, 5515, 161, 870, 2552, 5477, 3271, 1965, 4035, 1790, 2246, 3131, 1130, 4732, 3583, 1293, 1554, 4269, 2527, 3300, 5779, 3283, 4152, 5158, 1006, 62, 2303, 3067, 5481, 1426, 3346, 463, 2448, 1365, 1458, 4724, 2494, 4991, 5371, 4541, 5482, 2506, 1269, 5361, 2229, 3550, 2392, 2960, 2875, 5011, 1490, 2836, 467, 1222, 3394, 2658, 389, 2573, 4067, 3349, 6071, 2993, 3098, 5460, 4198, 260, 805, 5762, 4550, 5366, 5536, 1415, 2645, 768, 2992, 2902, 3235, 3017, 2370, 393, 1334, 1551, 5625, 4500, 2774, 5189, 3224, 5260, 4786, 3507, 972, 3970, 4331, 3526, 6687, 6368, 3335, 5913, 6452, 86, 5164, 1135, 4877, 4201, 2182, 4214, 2517, 2378, 4779, 729, 1625, 2949, 5711, 6057, 2711, 2094, 1929, 4969, 5236, 1063, 402, 4389, 5863, 5153, 3545, 4653, 3207, 4805, 88, 2500, 1737, 2304, 3153, 3383, 2117, 3245, 3963, 5709, 5812, 4492, 5483, 5830, 5564, 3484, 6733, 5800, 4043, 3943, 3161, 4543, 2181, 2510, 2136, 319, 4094, 3390, 5795, 669, 348, 512, 5928, 2338, 3572, 1119, 2812, 4838, 4090, 6552, 1201, 6444, 6674, 2115, 4665, 6082, 4469, 1674, 2216, 1692, 937, 2126, 2522, 1053, 5989, 5201, 4275, 122, 2876, 2433, 3661, 1661, 3536, 3872, 2005, 3448, 5502, 921, 6062, 1986, 4351, 3707, 5029, 5295, 6261, 3910, 2915, 1597, 4135, 1185, 6124, 4698, 6544, 2877, 824, 2999, 5347, 2080, 786, 6709, 2205, 1428, 6332, 5839, 5876, 6427, 2056, 6585, 4334, 5314, 6311, 2610, 2535, 2145, 4044, 4288, 2890, 5435, 5184, 1386, 3269, 2456, 801, 2307, 1268, 2169, 4753, 3721, 4827, 662, 6415, 6245, 270, 1844, 3129, 2657, 4772, 6509, 1480, 6618, 5735, 4189, 3977, 6652, 6375, 3544, 5850, 2481, 12, 5621, 2261, 959, 3191, 1410, 4215, 1134, 412, 5297, 4267, 1957, 4479, 4167, 5926, 3621, 5016, 3157, 6388, 1362, 783, 2936, 3806, 6743, 5690, 3758, 2647, 934, 272, 2393, 4394, 2539, 535, 2082, 4326, 5408, 4776, 6075, 2533, 6045, 5512, 3779, 3920, 2467, 5826, 6385, 4293, 4615, 1510, 1919, 4930, 3311, 6355, 5675, 2545, 919, 5485, 5058, 2710, 653, 4971, 5413, 6073, 995, 4407, 4994, 159, 457, 6354, 602, 3379, 664, 538, 1383, 3811, 3994, 4949, 3938, 6785, 771, 5315, 6363, 2778, 2074, 1895, 3724, 5587, 5288, 5676, 3800, 6572, 5935, 6063, 3016, 5707, 2581, 36, 615, 3410, 575, 4369, 2515, 2175, 2292, 495, 1514, 704, 5991, 4271, 4385, 3219, 294, 5974, 1793, 1890, 944, 298, 6668, 1772, 6587, 2009, 560, 906, 6226, 2248, 473, 1332, 860, 4076, 4160, 6165, 5188, 3503, 5548, 1171, 4738, 2387, 6258, 6512, 1685, 43, 5990, 6515, 2189, 2089, 1165, 469, 226, 967, 5144, 4304, 544, 1338, 2318, 2690, 204, 5894, 6606, 2007, 474, 2975, 4031, 6759, 3138, 1054, 2989, 4390, 6480, 6561, 4068, 899, 4312, 1912, 3563, 179, 2867, 1711, 5632, 2139, 3274, 4712, 2712, 2834, 4125, 3000, 3242, 4857, 5686, 5508, 450, 4562, 4404, 2371, 6575, 5798, 2028, 3678, 4410, 4705, 3716, 952, 5453, 4632, 3022, 5787, 684, 4082, 364, 5600, 4414, 3649, 3260, 6432, 6238, 3209, 5499, 3803, 5215, 1075, 2885, 3490, 4914, 3831, 4783, 2772, 1699, 3651, 1866, 6044, 189, 2209, 4354, 4374, 1090, 1461, 4053, 4235, 1295, 6611, 1813, 4990, 2488, 2242, 810, 6195, 5379, 6794, 2716, 4675, 4112, 5422, 6314, 5649, 6498, 1229, 5583, 383, 314, 765, 4380, 2632, 338, 5242, 3253, 5491, 4038, 1547, 3936, 1256, 6495, 4952, 788, 6140, 49, 1794, 4327, 6693, 5770, 1908, 4432, 3428, 720, 5869, 1982, 3307, 2254, 4303, 3151, 4793, 331, 4815, 1096, 1725, 241, 2127, 3386, 667, 663, 3495, 5696, 3345, 3356, 4510, 1934, 3210, 5641, 295, 5316, 4130, 2917, 2891, 3809, 3638, 1579, 4726, 54, 1571, 1877, 3579, 243, 6395, 4729, 6302, 1167, 2427, 1747, 2491, 6523, 4864, 3110, 2791, 3554, 4802, 6620, 4245, 4353, 3576, 3968, 2794, 1605, 304, 4370, 741, 5244, 6766, 3802, 2097, 625, 5193, 3031, 926, 6524, 2206, 1160, 1029, 519, 4942, 1287, 5181, 4923, 337, 4895, 4654, 2741, 2883, 2557, 4833, 10, 4669, 6478, 6546, 6356, 2330, 5791, 1979, 2911, 3491, 326, 880, 6202, 374, 2784, 5277, 4939, 340, 5241, 5693, 2561, 3921, 2054, 3912, 102, 5202, 1239, 2537, 2391, 5636, 1974, 3400, 4934, 1023, 4280, 1345, 4684, 1567, 430, 1413, 3546, 13, 6163, 5774, 4835, 3102, 4233, 4213, 2046, 2166, 2516, 2842, 4240, 2615, 2945, 5337, 5557, 4405, 5459, 1842, 902, 3051, 2473, 2962, 5799, 1752, 4508, 3892, 5873, 3820, 90, 4421, 4520, 5116, 5410, 4538, 4874, 5842, 1496, 6487, 6549, 6138, 5752, 893, 2013, 5230, 826, 2955, 3990, 1621, 2483, 3278, 1442, 2759, 6443, 2396, 4158, 2843, 6055, 5462, 2827, 2395, 4679, 6041, 4728, 2081, 3657, 2643, 2061, 59, 4502, 6714, 700, 405, 4292, 5552, 3071, 1166, 5601, 153, 3891, 119, 1271, 4487, 6479, 5892, 999, 3743, 2425, 5133, 3162, 4512, 3629, 4156, 1904, 2572, 3117, 5167, 6419, 1776, 5212, 6192, 6256, 61, 3176, 1132, 4820, 2748, 968, 1380, 927, 2908, 5037, 256, 1215, 623, 1022, 1371, 1805, 5525, 6292, 74, 1422, 387, 5998, 6637, 76, 5001, 2722, 2348, 3424, 890, 1759, 2738, 5748, 237, 6776, 3164, 4262, 1734, 6466, 5777, 143, 4812, 4111, 3687, 3009, 982, 1231, 4463, 6697, 3918, 5803, 606, 3233, 6129, 753, 691, 4946, 5698, 3799, 1865, 932, 1680, 5405, 6002, 503, 4010, 2627, 4041, 5194, 4944, 1640, 4810, 1937, 4791, 2288, 4643, 5019, 4522, 3202, 2439, 6301, 1840, 6455, 5362, 1662, 6493, 1398, 1463, 4750, 4453, 1122, 980, 4107, 1072, 1810, 2893, 3466, 6161, 4509, 4908, 3528, 6174, 3285, 665, 1438, 3827, 6162, 5804, 4710, 5027, 6335, 3197, 3570, 613, 6099, 5399, 1018, 5168, 1925, 6017, 94, 1596, 6738, 5421, 2177, 708, 6376, 5767, 1907, 1897, 2605, 4234, 4149, 2563, 4583, 817, 3525, 556]\n",
            "selected_features = ['cirrhosis', 'intubated_apache', 'aids', 'bun_apache', 'gcs_verbal_apache']\n",
            "{'gcs_verbal_apache <= 4.5': [{'bun_apache <= 24.5': [0, 1]}, 0]}\n",
            "selected_datas = [1663, 2401, 3938, 5566, 2835, 4393, 5626, 863, 4398, 5821, 3377, 2266, 5681, 1672, 1984, 3225, 6071, 5005, 1666, 2923, 792, 4003, 2903, 204, 1065, 1942, 6238, 3280, 3486, 2363, 5553, 766, 2211, 2647, 4764, 1357, 2580, 1706, 1319, 4773, 5049, 1406, 2346, 4658, 1041, 3294, 5146, 3093, 1854, 3394, 1504, 425, 6179, 4894, 6577, 3658, 238, 2202, 5097, 2132, 5600, 1307, 4056, 3237, 4898, 1717, 1974, 3169, 1349, 2730, 6604, 890, 2815, 5637, 2980, 1699, 3248, 6606, 4526, 5281, 331, 3828, 3203, 4170, 1676, 3475, 5617, 625, 3407, 831, 2286, 1868, 6538, 2738, 55, 1128, 1291, 1265, 3551, 3537, 2484, 3737, 3504, 4731, 5279, 6293, 4460, 5446, 3025, 1643, 268, 4823, 5975, 4987, 3343, 674, 361, 5945, 6627, 5982, 712, 5385, 2464, 2325, 6246, 6357, 4953, 4134, 6241, 2898, 6031, 1758, 4862, 5601, 4288, 220, 697, 6373, 2414, 3792, 767, 3599, 6571, 4302, 6079, 1893, 245, 599, 4693, 2468, 5104, 693, 2662, 6311, 5290, 4294, 2179, 4514, 1827, 447, 2373, 4444, 3334, 1695, 57, 6780, 477, 4621, 3206, 3638, 4431, 3308, 2089, 5619, 5483, 2067, 5889, 5181, 3595, 4975, 5134, 4504, 4007, 3775, 5256, 5788, 4346, 256, 5843, 368, 4454, 5083, 5920, 4548, 3873, 4179, 5753, 2078, 3789, 1560, 5852, 2831, 6625, 1922, 4985, 3743, 1146, 4472, 1073, 5739, 522, 3149, 3524, 1395, 5182, 3339, 3603, 3601, 515, 720, 5857, 2565, 6377, 2543, 4234, 5655, 5316, 3235, 1620, 4230, 4342, 2998, 5597, 3807, 524, 2216, 2573, 1012, 6432, 5350, 4683, 107, 1723, 1787, 5585, 748, 5417, 1040, 6446, 2895, 376, 6347, 4839, 6129, 807, 4935, 4564, 3416, 5283, 193, 5434, 4718, 787, 170, 3264, 3442, 6006, 6189, 6505, 478, 4382, 1841, 1279, 1623, 3669, 1913, 377, 5326, 5787, 3200, 243, 1619, 3863, 4078, 4614, 2337, 4419, 6398, 2295, 4912, 565, 1168, 3503, 86, 6072, 2635, 4962, 6720, 5580, 4921, 6034, 3384, 3644, 6749, 6705, 6634, 4989, 1489, 1016, 1548, 6581, 2452, 2819, 3346, 2012, 1076, 5077, 3452, 2632, 3315, 911, 2574, 4753, 943, 605, 1826, 1253, 5872, 5357, 679, 5409, 581, 5055, 1925, 4226, 527, 2243, 6163, 5607, 5559, 1659, 987, 1, 1351, 3545, 2607, 5690, 532, 3281, 6195, 1710, 3622, 623, 345, 5334, 3382, 1347, 2901, 2928, 3470, 1360, 1889, 2269, 6320, 5194, 2026, 901, 3682, 1811, 1093, 5236, 2323, 2547, 5963, 6716, 3604, 5871, 1421, 3469, 989, 1121, 347, 797, 5343, 1955, 3004, 3726, 6653, 3698, 6726, 3196, 5098, 6170, 6478, 2075, 87, 1719, 6278, 5468, 1303, 3702, 6289, 946, 2440, 1466, 1064, 466, 2178, 6044, 405, 3600, 6660, 1976, 1119, 2465, 1562, 1195, 2215, 6785, 3817, 745, 3351, 6600, 6695, 6362, 6078, 106, 4353, 569, 5708, 2407, 5026, 6144, 3398, 5878, 4076, 5086, 1220, 4826, 4867, 4184, 221, 1391, 6636, 1963, 2673, 5142, 3073, 2936, 904, 660, 2039, 6358, 4964, 1594, 851, 954, 1935, 3477, 5675, 1812, 4652, 497, 424, 3715, 5187, 4356, 5313, 2788, 5503, 3982, 5143, 6310, 258, 6146, 6060, 1872, 2172, 5029, 3861, 6212, 642, 617, 2144, 5937, 1465, 5731, 949, 394, 1485, 4066, 2880, 3489, 3478, 310, 3464, 4208, 5407, 5154, 4171, 4521, 5988, 5118, 211, 50, 3879, 4752, 3848, 6029, 4715, 803, 866, 3049, 5791, 6115, 5869, 2636, 1177, 4335, 6474, 2058, 2068, 6617, 3769, 1737, 1819, 4651, 2288, 274, 6685, 682, 6174, 2124, 3015, 1245, 4437, 3092, 680, 6559, 3333, 3162, 6525, 4796, 5324, 3673, 4218, 966, 3100, 339, 5190, 1648, 2336, 1499, 845, 5804, 1749, 5586, 6265, 6408, 1380, 1283, 5175, 1440, 439, 2444, 453, 770, 2109, 2352, 3883, 5763, 5426, 5914, 6312, 4149, 3516, 5550, 2873, 3324, 3107, 2732, 6454, 4174, 6638, 703, 5842, 545, 2540, 6751, 5876, 4657, 2778, 3148, 5599, 6268, 3578, 1788, 6387, 6138, 2224, 3341, 484, 5822, 2965, 4779, 5801, 1420, 1707, 336, 3843, 2930, 1947, 1970, 6227, 2524, 1505, 4207, 5284, 1920, 5979, 794, 6417, 2262, 2654, 2667, 5602, 6783, 5707, 2475, 5674, 3556, 4941, 3662, 5660, 4086, 5259, 2789, 780, 3121, 2787, 2418, 1579, 362, 1238, 2932, 6659, 6666, 2251, 1516, 3141, 11, 2076, 2047, 2991, 4115, 6097, 2117, 1567, 1688, 3152, 2111, 2433, 1733, 5729, 1588, 6232, 6731, 259, 2659, 6389, 2737, 598, 1668, 2844, 4997, 3988, 5930, 6148, 3758, 133, 4466, 6307, 2697, 6159, 6158, 4593, 178, 6155, 6795, 3028, 3705, 4648, 2590, 2309, 1887, 5336, 482, 6093, 3118, 4832, 332, 3947, 944, 1002, 4359, 3306, 3186, 1589, 6108, 4010, 3041, 4719, 5563, 3127, 282, 2133, 6166, 1302, 2638, 1214, 1386, 3224, 5622, 2762, 670, 5472, 5502, 781, 2480, 4141, 4413, 2696, 2268, 3800, 4072, 2053, 4443, 4048, 4403, 4409, 5462, 2146, 2904, 4565, 3770, 418, 3774, 4252, 5348, 379, 4597, 59, 464, 2775, 330, 2071, 3071, 810, 1105, 3458, 3697, 1268, 4306, 1669, 6288, 4304, 4436, 5006, 2678, 3221, 2979, 1364, 2125, 6324, 5676, 2033, 2961, 6607, 6393, 1348, 6693, 3161, 2709, 1522, 3935, 108, 2560, 2165, 5452, 3872, 4917, 2642, 5589, 4695, 1816, 2002, 5217, 3131, 1055, 4588, 5286, 4629, 3805, 4181, 4901, 6339, 3299, 3000, 2387, 1317, 2868, 2619, 6515, 855, 1571, 3143, 4297, 4523, 1025, 5514, 1086, 6104, 2694, 557, 3128, 6419, 2878, 2796, 4882, 3628, 5957, 3354, 2649, 5968, 4553, 6062, 4822, 3736, 2627, 3655, 4842, 4474, 962, 6026, 5611, 2765, 3319, 6336, 2473, 5784, 1747, 3279, 2134, 1176, 4425, 328, 2107, 2505, 3845, 1645, 2046, 5288, 513, 5237, 4880, 95, 445, 6423, 6597, 5984, 2745, 2302, 6594, 1170, 5205, 6098, 4958, 4549, 4088, 1675, 4438, 2946, 3890, 4389, 4646, 1379, 2769, 6700, 1734, 3360, 2531, 136, 4858, 490, 6414, 3577, 4774, 1140, 4434, 4365, 3408, 956, 4435, 2060, 4884, 2912, 4232, 4372, 3270, 2128, 2492, 915, 6272, 6191, 1533, 6028, 3884, 1713, 5372, 4937, 290, 6330, 1252, 3753, 4491, 4397, 5359, 2191, 5015, 52, 4416, 1834, 776, 771, 5227, 5198, 1529, 4314, 6015, 4728, 411, 5183, 1966, 4534, 1542, 1024, 6777, 4267, 3426, 1424, 1608, 3362, 3194, 4392, 702, 4282, 2343, 5165, 4307, 2442, 2349, 2818, 6734, 640, 531, 5688, 6040, 669, 5485, 4077, 5033, 4814, 5848, 1686, 1414, 2275, 6665, 5771, 1704, 65, 5540, 3667, 1992, 1460, 3949, 4227, 2884, 850, 104, 2794, 789, 3375, 3672, 4421, 1205, 4723, 3448, 3243, 6756, 1407, 1295, 4330, 1807, 6035, 3404, 6348, 3824, 3583, 4601, 5186, 4927, 5389, 3821, 1151, 4733, 3975, 2063, 5558, 1330, 4015, 2758, 4482, 1586, 5506, 5976, 1664, 128, 1018, 5669, 5803, 1369, 1123, 4610, 1795, 3818, 5335, 951, 6640, 3542, 159, 6404, 4040, 3266, 6251, 5301, 3724, 4854, 6645, 4681, 12, 5837, 5044, 1990, 5167, 6025, 134, 1469, 638, 5460, 2476, 1467, 1825, 3950, 1180, 1845, 1124, 2865, 2150, 6391, 6206, 1039, 1818, 2748, 3754, 1344, 5396, 1576, 5578, 6561, 2997, 4850, 6009, 6085, 3733, 4748, 4811, 1072, 135, 1382, 3389, 4799, 1563, 150, 1034, 2223, 3497, 5092, 5225, 5530, 3876, 5703, 4603, 4159, 43, 858, 6080, 2845, 2257, 1765, 4216, 6620, 6376, 3761, 3016, 3934, 1751, 2786, 6608, 4809, 4959, 4172, 3785, 4913, 3080, 3211, 56, 751, 1832, 5356, 1036, 6567, 3366, 3156, 5686, 1965, 3358, 1031, 6791, 2749, 283, 1397, 4737, 2358, 78, 6180, 3606, 5728, 5662, 559, 2445, 5533, 1033, 6622, 5823, 2430, 5525, 2613, 408, 31, 5786, 2583, 917, 2422, 3708, 6613, 2820, 124, 3973, 1029, 2705, 1842, 5117, 3793, 4915, 5604, 1771, 5192, 1638, 5344, 1082, 4245, 5522, 2098, 5706, 4265, 2021, 4165, 2425, 4366, 6017, 5593, 875, 5455, 4039, 369, 896, 2503, 5378, 6702, 6011, 2185, 4480, 4251, 2485, 4625, 102, 3750, 3656, 3898, 931, 3336, 5737, 5332, 3521, 4150, 3229, 6234, 101, 5347, 2790, 265, 629, 912, 1894, 4270, 5375, 434, 5101, 4844, 1943, 223, 1982, 6612, 3617, 3386, 6698, 224, 3236, 2205, 3888, 2206, 4085, 6413, 2771, 5184, 255, 4241, 4131, 3619, 4212, 3249, 4030, 246, 2311, 1729, 5740, 1632, 2449, 6149, 4064, 3466, 3870, 432, 5320, 6201, 4531, 5223, 1198, 5322, 3759, 6456, 2051, 6193, 812, 4847, 547, 1865, 100, 5073, 3966, 591, 373, 3440, 3573, 6137, 5579, 3957, 4496, 4220, 546, 5831, 4177, 1762, 5261, 4278, 860, 337, 3454, 163, 203, 5323, 2366, 6384, 3136, 633, 5444, 5661, 1028, 1037, 5442, 3304, 4380, 1583, 2344, 4967, 3040, 2101, 5892, 3111, 2123, 1817, 908, 5058, 822, 448, 6558, 2846, 6259, 4770, 4712, 6536, 2692, 3639, 5075, 568, 4193, 5882, 1521, 6302, 1931, 692, 1155, 4944, 1185, 4690, 964, 3215, 1408, 1323, 744, 2888, 4846, 1714, 6527, 6102, 5414, 3757, 6208, 2882, 5805]\n",
            "selected_features = ['pre_icu_los_days', 'gcs_eyes_apache', 'height', 'intubated_apache', 'sodium_apache']\n",
            "{'gcs_eyes_apache <= 2.5': [1, {'intubated_apache <= 0.5': [0, 1]}]}\n"
          ]
        }
      ],
      "source": [
        "forest = build_forest(training_data, n_trees, n_features, n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD0v4Af4bM_T"
      },
      "source": [
        "### Step 4: Make predictions with the random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "45zA7JFVbRLr"
      },
      "outputs": [],
      "source": [
        "def make_prediction_forest(forest, data):\n",
        "  \"\"\"\n",
        "  This function will use the pre-trained random forest to make the predictions\n",
        "  args:\n",
        "  * forest: the random forest\n",
        "  * data: the data used to predict\n",
        "  return:\n",
        "  * y_prediction: the predicted results\n",
        "  \"\"\"\n",
        "  y_prediction = []\n",
        "  predictions = []\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Loop through each tree in the forest\n",
        "  for tree in forest:\n",
        "    # Call 'make_prediction'\n",
        "    pred = make_prediction(tree, data)\n",
        "    predictions.append(pred)\n",
        "\n",
        "  # Here, each tree has made its predictions.\n",
        "  # We can use majority vote in which the final prediction is determined by the mode (most frequent prediction) across all the trees.\n",
        "  # Feel free to use any other method to determine the final prediction\n",
        "  \n",
        "  # Loop through each column of 'predictions'\n",
        "  for i in range(data.shape[0]):\n",
        "    # For a specific column, find out each tree's prediction\n",
        "    column_predictions = list(zip(*predictions))[i]\n",
        "    # Then, use a method to determine the final prediction for this column\n",
        "    # append the final prediction to y_prediction\n",
        "    if column_predictions.count(1) >= len(column_predictions) / 2:\n",
        "      y_prediction.append(1)\n",
        "    else:\n",
        "      y_prediction.append(0)\n",
        "  y_prediction = np.array(y_prediction)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "  return y_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXED6E837NRk"
      },
      "source": [
        "Validation (Optional)\n",
        "> If you split the data into training and validation sets in step 2, you can assess the accuracy of the forest here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "CsC39J9P7h-j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7717323327079425\n",
            "i = 2, t = 3, f = 10, s = 1360\n",
            "None\n",
            "best score = 1e-09\n",
            "i = 2, t = 3, f = 10, s = 1360\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "best_score = 1e-9\n",
        "best_parameter = []\n",
        "best_prediction = []\n",
        "# for i in range(1, 50, 2):\n",
        "    # for t in [2 * i + 1 for i in range(1, 10)]:\n",
        "        # for f in [i for i in range(3, training_data.shape[1])]:\n",
        "for i in range(2, 3):\n",
        "     for t in range(3, 4):\n",
        "        for f in [10]:\n",
        "            for s in [int(0.1 * i * training_data.shape[0]) for i in range(2, 3)]:\n",
        "                seed = i\n",
        "                n_trees = t\n",
        "                n_features = f\n",
        "                n_samples = s\n",
        "                forest = build_forest(training_data, n_trees, n_features, n_samples)\n",
        "                pred_validation = make_prediction_forest(forest, x_validation)\n",
        "                score = calculate_score(y_validation, pred_validation)\n",
        "                print(score)\n",
        "                print(print(f'i = {i}, t = {t}, f = {f}, s = {s}'))\n",
        "                if score >= 73.0:\n",
        "                    # best_score = score\n",
        "                    best_parameter = [i, t, f, s]\n",
        "                    best_prediction = pred_validation\n",
        "print(f'best score = {best_score}')\n",
        "print(f'i = {i}, t = {t}, f = {f}, s = {s}')\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqp8xzDJTV6S"
      },
      "source": [
        "After you have completed fine-tuning and validating the forest, you can proceed to make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "atRpa9KoPgNP"
      },
      "outputs": [],
      "source": [
        "y_pred_test = make_prediction_forest(forest, advanced_testing_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfLRewfzbjiZ"
      },
      "source": [
        "### Step 5: Write the Output File\n",
        "Save your predictions from the **random forest** in a csv file, named as **lab2_advanced.csv**\n",
        "> Note: Please do not touch the code in this step, we have made sure this outputs the correct file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "_H6MNjNmbst1"
      },
      "outputs": [],
      "source": [
        "advanced = []\n",
        "for i in range(len(y_pred_test)):\n",
        "  advanced.append(y_pred_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "7DHteTW7bvxz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     hospital_death\n",
              "Id                 \n",
              "0                 1\n",
              "1                 1\n",
              "2                 1\n",
              "3                 1\n",
              "4                 1\n",
              "..              ...\n",
              "895               0\n",
              "896               0\n",
              "897               1\n",
              "898               0\n",
              "899               0\n",
              "\n",
              "[900 rows x 1 columns]"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_path = 'lab2_advanced.csv'\n",
        "\n",
        "advanced_df = pd.DataFrame({'Id': range(len(advanced)), 'hospital_death': advanced})\n",
        "advanced_df.set_index('Id', inplace=True)\n",
        "advanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "AZWdWt5fGPe9"
      },
      "outputs": [],
      "source": [
        "advanced_df.to_csv(advanced_path, header = True, index = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
